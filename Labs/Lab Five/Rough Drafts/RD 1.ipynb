{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4: First Draft of Final Project**\n",
    "    - Salissa Hernandez\n",
    "    - Juan Carlos Dominguez\n",
    "    - Leonardo Piedrahita\n",
    "    - Brice Danvide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10 points]\n",
    "- Introduction and Related work: \n",
    "    - What are you doing and why is it important given what other researchers have done? \n",
    "    - That is, why does your topic investigate something new compared to other's work?\n",
    "    - What research questions do you plan to answer?\n",
    "\n",
    "- You will be graded on your ability to motivate the research topic. \n",
    "- Was the topic chosen interesting from a basic research or applied perspective?\n",
    "- This is typically about 2-3 minutes in length.\n",
    "- Be sure to cite references to related work here so that you can place your work in perspective to others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objective**\n",
    "\n",
    "This project investigates the application of **Reinforcement Learning (RL)** to the domain of **autonomous vehicles**, with a focus on enhancing safety and decision-making in dynamic driving environments. The analysis utilizes a **simulated highway environment** (via highway-env) to train an RL agent to perform fundamental tasks such as navigation, obstacle avoidance, and interaction with other vehicles. By implementing and evaluating different RL algorithms, the study aims to assess their effectiveness in facilitating real-time driving decisions and to explore their adaptability for real-world applications.\n",
    "\n",
    "The **primary objective** is to evaluate how RL can enable an autonomous vehicle to navigate safely and efficiently within a simulated environment and to compare the performance of various RL algorithms in managing complex driving scenarios. Additionally, the project examines the broader implications of RL in autonomous driving, including considerations of safety, efficiency, and ethics, with the goal of providing insights into challenges associated with deploying RL-based systems in real-world autonomous vehicles.\n",
    "\n",
    "### **Terminology Used in This Project**\n",
    "\n",
    "To ensure clarity and precision in the analysis, the following key terms are defined for use throughout the project:\n",
    "\n",
    "- **Reinforcement Learning (RL)**: A machine learning paradigm where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties based on its actions (Medium).\n",
    "- **Agent**: The autonomous vehicle in this context, which learns to navigate and make driving decisions based on environmental feedback and rewards (OpenAI Spinning Up).\n",
    "- **Environment**: The simulated world in which the agent operates, encompassing roads, traffic signs, other vehicles, pedestrians, and obstacles (OpenAI Spinning Up).\n",
    "- **State**: A specific configuration of the environment, including attributes such as the vehicle’s position, speed, distance to obstacles, or presence of other vehicles (OpenAI Spinning Up).\n",
    "- **Action**: The decision executed by the agent in a given state, such as steering left, accelerating, or braking (OpenAI Spinning Up).\n",
    "- **Reward**: A numerical value assigned to the agent’s actions, guiding it toward desirable behaviors, such as avoiding collisions or adhering to traffic regulations (OpenAI Spinning Up).\n",
    "- **Deep Q-Networks (DQN)**: RL algorithm designed to estimate expected future rewards for each action, enabling optimal decision-making in specific states (HuggingFace).\n",
    "- **Proximal Policy Optimization (PPO)**: A state-of-the-art RL algorithm suited for training in environments with large action spaces, such as controlling a vehicle’s speed and direction (HuggingFace).\n",
    "- **Advantage Actor-Critic (A2C)**: A synchronous variant of the Actor-Critic algorithm that uses two neural networks—one to predict the policy (actor) and another to estimate the value function (critic). A2C helps balance exploration and exploitation and can be effective in environments where stability and sample efficiency are important (OpenAI Spinning Up, HuggingFace).\n",
    "\n",
    "**Sources**:\n",
    "\n",
    "- Medium: https://medium.com/%40gurkanc/deep-reinforcement-learning-agents-algorithms-and-strategies-a-practical-game-scenario-a412428ae0e0  \n",
    "- OpenAI Spinning Up - Intro to RL: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html  \n",
    "- HuggingFace (DQN): https://huggingface.co/learn/deep-rl-course/en/unit3/from-q-to-dqn  \n",
    "- HuggingFace (PPO): https://huggingface.co/blog/deep-rl-ppo  \n",
    "- HuggingFace (A2C): https://huggingface.co/blog/deep-rl-a2c\n",
    "\n",
    "**References**:\n",
    "\n",
    "- https://github.com/Farama-Foundation/HighwayEnv?tab=readme-ov-file\n",
    "- https://github.com/TeyKra/Reinforcement-Learning-HighwayEnv/blob/main/Reinforcement%20Learning%20HighwayEnv.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Motivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Motivation for Our Research Project**\n",
    "\n",
    "Autonomous vehicles (AVs) represent a transformative shift in the transportation industry, offering potential for enhanced safety, accessibility, and efficiency. Despite progress from industry leaders such as Waymo and Tesla, AVs face significant challenges in consistently making safe, efficient, and ethically sound decisions in real time. These systems must navigate dynamic environments characterized by unpredictable traffic patterns, diverse road conditions, variable weather, and interactions with pedestrians and other vehicles.\n",
    "\n",
    "Reinforcement learning (RL) provides a promising framework for optimizing AV decision-making through experience-based learning. Unlike rule-based or supervised approaches, RL agents learn optimal policies by interacting with their environment, enabling flexible and adaptive responses to novel situations. Research by Aradi (2020) suggests that RL can improve decision-making in dynamic driving environments by continuously refining actions based on feedback from the environment. This study investigates how RL can enhance AV performance in terms of safety, efficiency, and adaptability, with the aim of developing robust navigation strategies that reduce accident rates and improve traffic flow.\n",
    "\n",
    "The **motivation behind this project is to investigate how RL can improve AVs’ performance in terms of safety and efficiency**. Specifically, we evaluate whether RL can improve two critical operational outcomes for AVs: **reducing collision rates** (a safety measure) and **minimizing travel time** (a measure of efficiency). These are both direct measures of decision-making quality, and by focusing on them, we aim to evaluate the practical viability of RL in supporting safe and efficient autonomous highway driving. By leveraging RL algorithms, we aim to assess whether these algorithms can generalize across varied driving scenarios, leading to more robust navigation strategies, reduced collision rates, and improved overall traffic efficiency.\n",
    "\n",
    "The societal impetus for this research stems from the need to mitigate the high incidence of accidents caused by human error, which accounts for over 90% of vehicular crashes according to the National Highway Traffic Safety Administration (NHTSA). AVs have the potential to eliminate risks associated with distraction, fatigue, and impaired judgment, while also improving mobility for individuals unable to drive and reducing emissions through optimized driving patterns.  In addition, optimized travel efficiency can help reduce emissions and congestion, enhancing the broader benefits of AV adoption. This project contributes to the advancement of autonomous technologies by exploring RL as a scalable and adaptive framework for intelligent decision-making in complex driving environments.\n",
    "\n",
    "**References:**\n",
    "- Aradi, S. (2020). Survey of deep reinforcement learning for motion planning of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 22(6), 3023-3035.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Importance of the Investigation**\n",
    "\n",
    "This research addresses critical challenges in autonomous driving: ensuring safety and effiency in uncertain environments. Current AV systems, such as Tesla’s Autopilot, Waymo’s Driver, and Cruise’s Origin platform, have made significant strides but struggle with emergent, high-stakes scenarios, including sudden pedestrian crossings, dynamic lane changes, traffic signal malfunctions, construction detours, and adverse weather conditions. High-profile incidents highlight these limitations. For instance, in 2023, Cruise suspended operations nationwide after a San Francisco incident where a vehicle failed to yield appropriately, dragging a pedestrian following a collision. Similarly, Tesla’s Autopilot has faced National Highway Traffic Safety Administration (NHTSA) investigations for collisions with stationary emergency vehicles and phantom braking, while Waymo’s overly cautious behavior has disrupted traffic flow.\n",
    "\n",
    "These issues stem from the rigidity of deterministic or supervised-learning-based frameworks, which rely on predefined rules or labeled datasets and struggle to generalize to rare or novel situations. RL offers a compelling alternative by allowing agents to iteratively refine behavior through environmental feedback, learning nuanced behaviors—such as yielding to aggressive drivers or adjusting speed in low-visibility conditions—that are difficult to hand-code or annotate. Aradi (2020) highlights how RL can handle these complex, unpredictable environments more effectively than traditional rule-based systems.\n",
    "\n",
    "The investigation is timely given the growing deployment of AVs on public roads, where safety is paramount. By evaluating RL algorithms in terms of collision rates and travel time as primary metrics, this study directly addresses two of the most urgent and tangible goals for safe, scalable AV deployment. The findings aim to provide early evidence for the viability of RL in improving critical outcomes under realistic highway conditions.\n",
    "\n",
    "**References** \n",
    "- National Highway Traffic Safety Administration (NHTSA). (2015). Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey.\n",
    "- Tesla AI Team. (2021). Tesla Autonomy Day / AI Day. https://www.tesla.com/AI\n",
    "- Waymo Safety Report. (2020). https://waymo.com/safety\n",
    "- Aradi, S. (2020). Survey of deep reinforcement learning for motion planning of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 22(6), 3023-3035.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Related Work**\n",
    "\n",
    "Significant progress has been made in applying reinforcement learning (RL) and related machine learning techniques to autonomous vehicle systems. Industry leaders like Tesla and Waymo have developed real-time decision-making frameworks using deep neural networks. Tesla’s Autopilot employs end-to-end deep learning for lane detection, object classification, and lane-changing maneuvers, while Waymo’s modular architecture integrates deep learning and imitation learning to navigate complex urban environments (Tesla AI Team, 2021; Waymo Team, 2020).\n",
    "\n",
    "In academic research, simulation environments are widely used to train RL agents due to safety and cost constraints. The **highway-env** simulator, a lightweight, lane-based platform, supports high-level decision-making in multi-agent traffic scenarios, enabling rapid prototyping compared to high-fidelity simulators like CARLA (Dosovitskiy et al., 2017). RL algorithms such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO) have shown success in tasks like lane changing, collision avoidance, and maintaining safe distances under dynamic conditions (Mnih et al., 2015; Schulman et al., 2017).\n",
    "\n",
    "Foundational work from DeepMind in deep RL, particularly in game-playing and robotic control, has highlighted challenges in generalization, sample efficiency, and policy transfer, which are directly relevant to AVs navigating variable environments (Silver et al., 2016). This study builds on these efforts by evaluating RL algorithms in simulated driving scenarios with unpredictable elements, such as sudden pedestrian crossings or traffic disruptions. Unlike prior work focused on optimal performance in controlled settings, this project emphasizes understanding **generalization limits** and **failure modes** of RL policies under out-of-distribution events, critical for assessing RL’s viability in safety-critical, real-world deployments.\n",
    "\n",
    "**References**\n",
    "- Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. *Conference on Robot Learning (CoRL)*.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.\n",
    "- Silver, D., Huang, A., Maddison, C. J., et al. (2016). Mastering the game of Go with deep neural networks and tree search. *Nature*.\n",
    "- Tesla AI Team. (2021). *Tesla AI Day 2021*. https://www.tesla.com/AI\n",
    "- Waymo Team. (2020). *On the Road to Fully Autonomous Driving*. https://waymo.com/research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Main Research Questions**\n",
    "\n",
    "This research investigates how reinforcement learning (RL) can enhance autonomous vehicle (AV) decision-making in complex, real-world environments. The following questions guide the inquiry:\n",
    "\n",
    "1. **How can reinforcement learning improve autonomous vehicle decision-making in highway environments, as measured by collision rates and travel time?**  \n",
    "    In highway driving, AVs must make rapid decisions while interacting with other vehicles—merging, overtaking, maintaining safe distances, and responding to dynamic traffic conditions. This study evaluates whether RL-based control systems can make safer and more efficient decisions compared to fixed-policy baselines. Specifically, the focus is on two key performance metrics: collision rates (as a measure of safety) and travel time (as a measure of efficiency) in simulated highway scenarios using the highway-env environment.\n",
    "\n",
    "2. **How well do RL agents trained under specific highway conditions generalize to unseen environments, such as increased traffic density and other aggressive driver behaviors?**  \n",
    "    High performance in simulation does not guarantee success in real-world applications. This question investigates the \"reality gap\", which refers to how well RL policies learned in a controlled simulation environment can generalize to new and unseen real-world conditions. The study will examine how agents trained in less complex scenarios perform when faced with increased traffic density and aggressive driver behaviors, testing the agents' robustness and adaptability to novel conditions.\n",
    "\n",
    "\n",
    "These questions structure the investigation into how RL can serve as a foundation for intelligent, adaptable, and trustworthy autonomous driving systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5 Hypothesis**\n",
    "\n",
    "This study hypothesizes that **reinforcement learning (RL) algorithms can significantly enhance the real-time decision-making capabilities of autonomous vehicles in terms of reducing collision rates and minimizing travel time by enabling agents to learn from environmental interactions and iteratively refine their behavior over time.**\n",
    "\n",
    "Specifically, the following outcomes are anticipated:\n",
    "- **Deep Q-Networks (DQN)** are expected to be effective for discrete, task-specific behaviors commonly observed in highway-env scenarios, such as lane keeping, overtaking, and collision avoidance. DQN’s ability to approximate Q-values in high-dimensional observation spaces supports the optimization of long-term safety and efficiency in structured, rule-based traffic environments.\n",
    "- **Proximal Policy Optimization (PPO)** is predicted to demonstrate superior performance in more complex or adaptive driving tasks, particularly in environments with continuous action components (e.g., speed modulation and strategic decision-making). PPO’s stability and robustness across policy updates make it well-suited for learning consistent, safe behavior under dynamic multi-agent conditions.\n",
    "- **Advantage Actor-Critic (A2C)** is anticipated to provide a balance between performance and computational efficiency. As a synchronous actor-critic algorithm, A2C uses parallel environments to stabilize learning and is expected to perform well in moderately complex driving scenarios. Its ability to concurrently learn value estimation and policy behavior may lead to effective policy refinement under time constraints or limited training steps.\n",
    "\n",
    "RL-trained agents are expected to outperform traditional rule-based baselines on key performance metrics, including collision rate and travel time efficiency (e.g., time to reach a destination). Additionally, these agents are anticipated to exhibit adaptive behavior, such as adjusting speed in response to aggressive drivers or rerouting around obstacles, under variations in traffic density, driver aggressiveness, and environmental layout.\n",
    "\n",
    "While RL agents are expected to perform well in controlled simulations, their ability to generalize to real-world conditions may be limited. Simulation environments such as highway-env provide valuable platforms for experimentation but lack the sensory noise, complexity, and unpredictability of real-world driving. Bridging this “reality gap” will likely require mechanisms such as domain randomization, hybrid control schemes, or transfer learning techniques.\n",
    "\n",
    "Overall, this study posits that reinforcement learning offers a promising framework for initial investigation into intelligent and adaptive autonomous driving. However, its effective deployment will depend on addressing persistent challenges related to generalization, safety, and real-world robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10 points] \n",
    "- Methods: \n",
    "    - This part of the presentation should present you methods both intuitively and with enough depth that the audience can understand how you approached investigating your research questions. \n",
    "    - Diagram and equations are expected, along with intuitive explanations of the approach.\n",
    "\n",
    "- You will be graded on your ability to convey the mathematics or intuition behind a new algorithm, and explain your methodology for analyzing the data (typically with a figure). \n",
    "- Were the analyses appropriate for what was investigated?\n",
    "- This is typically about 3-5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Methodology & Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Methodology & Analysis**\n",
    "\n",
    "To investigate how reinforcement learning (RL) can enhance autonomous vehicle decision-making in terms of **reducing collision rates** and **minimizing travel time**, this study conducts experiments using **highway-env**, a lightweight, configurable simulation environment tailored for high-speed, multi-agent driving scenarios. This environment supports diverse traffic configurations, driver behaviors, and lane-based interactions, making it suitable for controlled yet realistic testing of RL agents. Unlike high-fidelity simulators reliant on detailed graphics or physics, highway-env emphasizes strategic decision-making dynamics—such as lane changes, overtaking, and safe distance maintenance—facilitating rapid prototyping and iteration.\n",
    "\n",
    "The study employs three RL algorithms: Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Advantage Actor-Critic (A2C). DQN leverages deep learning to handle high-dimensional observations and learn optimal policies for discrete action spaces. PPO, an advanced on-policy algorithm, offers stability and sample efficiency, ideal for dynamic, multi-agent environments. A2C, included as an exploratory method, combines value-based and policy-based approaches for stable learning in continuous action spaces.\n",
    "\n",
    "To address the **first research question—how RL can improve decision-making in highway environments, as measured by collision rates and travel time**—we simulate common highway driving scenarios such as dense traffic, aggressive driver behaviors, lane merging, and dynamic speed changes. RL agents are trained to maximize a reward function that encourages safe and efficient driving behaviors. The reward function includes penalties for collisions and incentives for maintaining safe distances, adjusting speed appropriately, and minimizing travel time. The agents’ performance is evaluated based on the collision rate (number of collisions during simulation) and travel time (time taken to reach the destination safely). The expectation is that RL algorithms, especially PPO and DQN, will demonstrate superior performance over rule-based systems, reducing both collisions and travel time as the agent learns to navigate the highway more efficiently.\n",
    "\n",
    "For the **second research question—how well RL agents generalize to new highway conditions, such as increased traffic density and aggressive driver behaviors**—the agents trained in controlled conditions are tested in novel environments. These environments vary key parameters like traffic density, where more vehicles are introduced, and driver aggressiveness, where other vehicles may frequently change lanes or attempt to overtake. Agents are also tested under different initial conditions, such as varying road layouts or traffic flows. The agents are evaluated in these new conditions without further training, and their ability to adapt is measured by success rate (how often the agent reaches the destination safely) and behavioral drift (how much the agent’s behavior deviates from the learned policy). Additionally, we measure stability, which refers to the consistency of the agent’s decision-making under the new conditions.\n",
    "\n",
    "Throughout the study, data such as learning curves, action distributions, trajectory visualizations, and video rollouts are logged and analyzed. This combination of algorithm benchmarking, generalization testing, and stress analysis provides a comprehensive foundation for understanding RL’s strengths, limitations, and practical potential in autonomous highway driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Environment Setup**\n",
    "\n",
    "Experiments are conducted using **highway-env**, a Python-based simulation environment designed for testing autonomous driving agents in highway scenarios. This environment offers a simplified yet effective abstraction of real-world driving, simulating multi-lane roads, interacting vehicles, varying traffic densities, and driver behaviors. It supports configurable parameters, including lane numbers, vehicle dynamics, and traffic flow, enabling diverse training and testing conditions that reflect high-speed highway complexities.\n",
    "\n",
    "The highway-env simulator facilitates both structured tasks, such as lane keeping and destination arrival, and dynamic challenges, like overtaking slower vehicles, avoiding collisions, and responding to sudden traffic changes. Its fine-grained control over parameters and real-time visual feedback support iterative RL development. The environment’s lightweight design allows training over thousands of episodes without the computational burden of high-resolution graphics or physical modeling, ensuring scalable, safe, and repeatable experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Reinforcement Learning Algorithms**\n",
    "\n",
    "The study implements the following RL algorithms for comparative analysis:\n",
    "\n",
    "- **Deep Q-Networks (DQN)**: An RL algorithm leveraging deep neural networks to approximate action-value functions in high-dimensional state spaces. DQN employs experience replay and target networks to stabilize training, effective for sequential decision-making and perception-based control. The Q-function is approximated as:\n",
    "\n",
    "  $$Q(s_t, a_t; \\theta) = \\mathbb{E}[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta^-)]$$\n",
    "\n",
    "  Where $\\theta $ represents the parameters of the Q-network, and $\\theta^-$ are the parameters of the target network.\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)**: A state-of-the-art policy gradient method known for sample efficiency and training stability, particularly in continuous action environments like vehicle control, where fine-grained steering and speed adjustments are required. The objective function for PPO is:\n",
    "\n",
    "  $$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t \\left[ \\min\\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "  Where $r_t(\\theta)$ is the probability ratio, $\\hat{A}_t$ is the advantage estimate, and $\\epsilon$ is the clipping parameter.\n",
    "\n",
    "- **Advantage Actor-Critic (A2C)**: An exploratory policy gradient method combining value-based and policy-based approaches, using an actor to select actions and a critic to evaluate them. A2C is effective in continuous action spaces and supports stable learning. The update rule for the advantage function is:\n",
    "\n",
    "  $$L^{A2C}(\\theta, \\phi) = \\hat{\\mathbb{E}}_t \\left[ \\log \\pi_\\theta(a_t|s_t) \\hat{A}_t - \\beta \\left( \\| \\nabla_\\theta \\pi_\\theta(a_t|s_t) \\|^2 \\right) \\right]$$\n",
    "\n",
    "  Where $\\hat{A}_t$ is the advantage estimate, and $\\beta$ is the regularization term for the entropy of the policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Experimental Tasks**\n",
    "\n",
    "To assess algorithm generalizability and robustness, agents are evaluated on a progression of tasks:\n",
    "\n",
    "1. **Basic Lane-Following**: Tests stability and path adherence under ideal conditions.\n",
    "2. **Dynamic Vehicle Interaction**: Introduces multi-agent scenarios with varying vehicle speeds and density, requiring safe merging, overtaking, and adaptive responses.\n",
    "3. **Traffic Rule Compliance**: Evaluates adherence to norms like lane discipline, safe following distances, and yielding behavior.\n",
    "4. **Environmental Perturbation**: Tests policy robustness under reduced visibility, aggressive drivers, and modified road layouts.\n",
    "\n",
    "Agents are trained on a subset of tasks and tested on both seen and unseen conditions to measure generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Evaluation Metrics**\n",
    "\n",
    "The analysis employs quantitative and qualitative metrics:\n",
    "\n",
    "- **Safety**: Number of collisions, near-misses (e.g., vehicles within a 2-meter threshold), and traffic violations per episode.\n",
    "- **Efficiency**: Time-to-completion and distance traveled to reach destinations.\n",
    "- **Reward Accumulation**: Cumulative reward across training and evaluation episodes.\n",
    "- **Policy Robustness**: Percentage drop in reward or success rate in new or perturbed environments.\n",
    "- **Convergence Behavior**: Training stability and learning curves across episodes.\n",
    "\n",
    "These metrics enable comparison of raw performance, reliability, and adaptability across RL approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6 Analysis Strategy**\n",
    "\n",
    "The study conducts **intra-algorithm analysis** (evaluating each algorithm’s performance across tasks and conditions) and **inter-algorithm comparison** (benchmarking algorithms against one another). Visualizations, including reward curves, policy heatmaps, and behavioral traces, illustrate differences in decision-making behavior.\n",
    "\n",
    "Qualitative insights, such as agent hesitation at intersections or failures under occlusion, are recorded through visual inspection and frame-by-frame analysis of edge cases. These findings identify policy limitations and suggest improvements, such as curriculum learning, hybrid models, or safety constraints. This comprehensive approach evaluates RL’s practical potential for autonomous vehicle decision-making in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10 points] \n",
    "- Results and conclusions: \n",
    "    - The overall results should be presented visually with enough explanation to understand relevance. \n",
    "    - Did you answer your research questions? \n",
    "    - What is the takeaway and contribution to machine learning?\n",
    "\n",
    "- You will be graded on the discussion of your results including the explanation of figures, summary of findings, and implication of the result.\n",
    "- You should also motivate chosen results metrics.\n",
    "- How does this answer your research question? \n",
    "- Are there any limitations to what you can conclude? \n",
    "- This section is typically about 3 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Results & Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Analysis of RQ 1: How can reinforcement learning improve autonomous vehicle decision-making in highway environments, as measured by collision rates and travel time?**\n",
    "\n",
    "This section investigates how reinforcement learning (RL) can enhance decision-making in terms of collision rates and travel times for autonomous vehicles in high-speed highway environments, addressing the first research question. The study compares the behavior and performance of a trained DQN agent against an untrained baseline in the **highway-env** simulator, focusing on improvements in safety, efficiency, and adaptability.\n",
    "\n",
    "The analysis is supported by two key visualizations: a training reward curve, which tracks the DQN agent’s learning progression over episodes, and a bar plot comparing the mean rewards of the untrained and trained agents. These visualizations, detailed in the following subsections, provide both quantitative and qualitative evidence of RL’s impact on decision-making. The training reward curve illustrates the agent’s ability to learn optimal behaviors over time, while the performance comparison highlights the trained agent’s superior reward accumulation and safer driving strategies compared to the untrained agent’s random actions.\n",
    "\n",
    "The results demonstrate that RL enables the DQN agent to iteratively refine its decision-making through environmental interactions, aligning with the hypothesis that RL can significantly enhance real-time decision-making capabilities in autonomous vehicles. However, variability in performance suggests areas for further refinement, such as additional training or reward shaping, which also informs the fourth research question regarding RL’s limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Modules & Libraries\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "import shutil\n",
    "import gc\n",
    "import torch\n",
    "import highway_env \n",
    "import gymnasium as gym\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, BaseCallback\n",
    "from moviepy import VideoFileClip, ColorClip, concatenate_videoclips\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import HTML\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "from tabulate import tabulate \n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Analysis of RQ 2: How well do RL agents trained under specific highway conditions generalize to unseen environments, such as increased traffic density and other aggressive driver behaviors?**\n",
    "This section takes a closer look at how well a DQN agent, trained in a controlled highway setting, can handle a much more chaotic and unpredictable environment. The test scenario ramped up the traffic density, widened the roads, introduced more aggressive driving behaviors, and implemented a stricter reward system to mimic conditions that are quite different from what the agent was originally trained on. The goal here was to see if the agent could apply its learned driving strategies to these unfamiliar and dynamic situations, which would help us understand just how robust and adaptable reinforcement learning can be when it comes to controlling autonomous vehicles.\n",
    "\n",
    "The results showed that while the agent was able to adapt to some degree—maintaining essential behaviors like staying in its lane and avoiding collisions—it also displayed moments of hesitation and overly cautious choices in high-risk scenarios. This indicates that, although the trained policy can manage increased complexity to a certain extent, its effectiveness is quite sensitive to the differences between the training and real-world environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highway_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
