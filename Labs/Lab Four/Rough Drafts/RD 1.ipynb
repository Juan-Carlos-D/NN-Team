{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4: First Draft of Final Project**\n",
    "    - Salissa Hernandez\n",
    "    - Juan Carlos Dominguez\n",
    "    - Leonardo Piedrahita\n",
    "    - Brice Danvide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is meant to be the first draft of motivation, methods, and one result for the final project of the class. In this lab, you will select a topic to investigate and perform an analysis to understand if this topic is of sufficient interest to use as a final project. You will be writing down many of these aspects in preparation for a final project presentation. \n",
    "\n",
    "For the final class project, you should investigate a topic from the course of your choosing. This topic can be related to anything we have discussed. For instance, you might choose to investigate a new implementation of Stable Diffusion training with an alternative latent space function (or new form of cross attention). Please do not let this example bias your choosing of a project--the topic you choose to investigate does not need to be a new algorithmic approach. It could be a new application of an algorithm we discussed. For instance, you might choose to use multi-task modeling for assessing robotic surgery. Or you might choose a new topic in ethical considerations of models. The only requirement is that the idea be something that creates new knowledge in the world and is somehow related to the vast number of topics we discussed in the class. \n",
    "\n",
    "Any topic can be chosen from ethical machine learning, convolutional visualization, data generation (with VAE's or GANs), multi-task or multi-modal architectures, stable diffusion, style transfer, or reinforcement learning. \n",
    "\n",
    "This lab will help you to ensure that the topic is appropriate for a final project in the course in terms of scope (not so easy that it might considered trivial, but not so hard that it might be considered a full blown dissertation). The right aim of scope should be such that you are investigating the initial analysis of a research topic--but there would still be much work to do for a full research publication. If you are unsure if your topic is appropriate, please contact the instructor for feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objective**\n",
    "For our final project, we explore the application of **Reinforcement Learning** (RL) to the domain of **autonomous vehicles**, focusing on how RL can improve safety and decision-making processes in dynamic driving environments. Our analysis involves using a **simulated driving environment** (such as CARLA) to train an RL agent to perform basic tasks like navigation, obstacle avoidance, and interaction with other vehicles. By implementing and testing different RL algorithms, we seek to understand their effectiveness in making real-time driving decisions and to analyze how these algorithms can be adapted for real-world applications.\n",
    "\n",
    "Our **primary objective** is to determine how RL can be used to teach an autonomous vehicle to navigate safely and efficiently within a simulated environment, and to assess the performance of different algorithms in handling complex driving scenarios. Additionally, we will explore the broader implications of RL in autonomous driving, including safety, efficiency, and ethics, with the goal of providing insights into potential challenges when deploying RL-based systems in real-wrold autonomous vehicles. \n",
    "\n",
    "### **Terminology Used in This Project**\n",
    "To ensure clarity and understanding in our analysis, we defined the following key terms use throughout our project:\n",
    "- Reinforcement Learning (RL): A type of machine learning where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties based on its actions (Medium)\n",
    "- Agent: The autonomous vehicle in this scenario, which learns to navigate and make driving decisions based on the environment and rewards. (OpenAI Spinning Up)\n",
    "- Environment: The simulated world in which the agent operates, which includes roads, traffic signs, other vehicles, pedestrians, and obstacles (OpenAI Spinning Up)\n",
    "- State: A specific configuration of the environment, such as the position and speed of the vehicle, the distance to obstacles, or the presence of other cars. (OpenAI Spinning Up)\n",
    "- Action: The decision made by the agent at a given state, such as steering left, speeding up, or braking. (OpenAI Spinning Up)\n",
    "- Reward: A value assigned to the agent's actions, which helps it learn which behaviors are desirable (e.g., avoiding collisions or following traffic laws). (OpenAI Spinning Up)\n",
    "- Q-Learning / Deep Q-Networks (DQN): RL algorithms used to estimate the expected future rewards for each action, helping the agent make optimal decisions in a given state. (HuggingFace)\n",
    "- Proximal Policy Optimization (PPO): A state-of-the-art RL algorithm used for training in environments with large action spaces, such as controlling a vehicle's speed and direction. (HuggingFace)\n",
    "\n",
    "Sources:\n",
    "- Medium: https://medium.com/%40gurkanc/deep-reinforcement-learning-agents-algorithms-and-strategies-a-practical-game-scenario-a412428ae0e0\n",
    "- OpenAI Spinning Up - Intro to RL: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
    "- HuggingFace: https://huggingface.co/learn/deep-rl-course/en/unit3/from-q-to-dqn\n",
    "- HuggingFace: https://huggingface.co/blog/deep-rl-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Motivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- Motivate the need for the research project. \n",
    "- Why is this investigation important? \n",
    "- What related work are you building from? \n",
    "- What are the main research question(s)? \n",
    "- What is your hypothesis for what will happen? \n",
    "- This section should be something that can be converted into two or three slides for the final presentation. \n",
    "- You should write down the motivations and related work that will be presented to the instructor later on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Motivation for Our Research Project**\n",
    "Autonomous vehicles (AVs) represent one of the most transformative innovations in the modern transportation industry. While the technological advancements are promising from current projects like Waymo, the ability of AVs to make safe, efficient, and ethical driving decisions in real-time remains a critical challenge. AVs operate in highly dynamic environments with unpredictable conditions such as varying traffic patterns, road conditions, weather, and interactions with pedestrians and other vehicles. Reinforcement learning offers an opportunity to optimize the decision-making process through experience-based learning.\n",
    "\n",
    "The **motivation behind this project is to investigate how RL can improve AVs’ performance in terms of safety, efficiency, and adaptability**. By utilizing RL algorithms, we hope to enhance the vehicle’s ability to learn from its environment and make intelligent decisions, such as when to accelerate, brake, and avoid obstacles. These algorithms can also learn from different driving scenarios, enabling the vehicle to perform better in diverse real-world conditions, potentially reducing accidents and improving traffic flow.\n",
    "\n",
    "The need for autonomous vehicles stems from a broader societal context. Human error accounts for the overwhelming majority of traffic accidents, and autonomous systems offer the potential to significantly reduce these numbers by eliminating distractions, fatigue, and impaired judgment. Moreover, AVs can provide mobility for individuals who are unable to drive, increase transportation efficiency through intelligent routing and coordination, and contribute to environmental sustainability by reducing emissions through smoother, optimized driving. As we move toward a future where AVs are more widely deployed, it becomes increasingly important to ensure these systems are capable of making real-time, intelligent decisions in complex environments. This project contributes to that goal by exploring reinforcement learning as a scalable, adaptable framework for safe and intelligent autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Why is this investigation important?**\n",
    "The importance of this research lies in addressing two of the most pressing challenges in the field of autonomous driving: safety and real-time decision-making in uncertain environments. While modern autonomous vehicle (AV) systems—such as Tesla’s Autopilot, Waymo’s Driver, and Cruise’s Origin platform—have demonstrated considerable progress, they remain limited in their ability to respond effectively to emergent, high-stakes scenarios. These include sudden pedestrian crossings, dynamic lane changes, traffic signal malfunctions, construction detours, and adverse weather conditions like heavy rain or fog. Several high-profile incidents underscore this limitation.\n",
    "\n",
    "In 2023, Cruise paused all autonomous operations across the United States after an incident in San Francisco where one of its vehicles failed to yield appropriately and dragged a pedestrian following a collision caused by another car. Similarly, Tesla’s Autopilot and Full Self-Driving (FSD) systems have faced multiple investigations by the National Highway Traffic Safety Administration (NHTSA) due to collisions with stationary emergency vehicles and phantom braking events. Even Waymo, considered one of the most technically mature systems, has encountered issues with overly cautious behavior—such as freezing in intersections or failing to merge—causing confusion for human drivers and traffic flow disruptions.\n",
    "\n",
    "These failures are rooted in the rigidity of current AV decision-making frameworks, which are largely deterministic or supervised-learning-based. Such systems rely on predefined rules or behavior learned from labeled datasets, limiting their ability to generalize to rare or novel situations. As a result, they are often brittle, overly conservative, or prone to misclassification under conditions that diverge from their training data.\n",
    "\n",
    "Reinforcement learning (RL) offers a compelling alternative by enabling agents to learn optimal policies through interaction and feedback. Rather than relying solely on human-designed heuristics, RL-based systems iteratively refine their behavior by observing the long-term consequences of their actions. This allows the AV to learn nuanced driving behaviors—such as yielding to aggressive drivers, dynamically rerouting around construction, or adjusting speed in low-visibility weather—that would be difficult to hand-code or annotate in large-scale datasets.\n",
    "\n",
    "The relevance of this investigation is further underscored by the increasing push for AV deployment on public roads, where safety is a primary concern. According to the National Highway Traffic Safety Administration (NHTSA), over 90% of vehicular accidents are attributable to human error. As AVs are introduced into mixed traffic ecosystems, their ability to make adaptive, ethical, and reliable decisions in real time becomes critical—not only for reducing accidents but also for gaining public trust and regulatory approval.\n",
    "\n",
    "This research aligns with the broader shift in the AV industry from rigid, rule-driven systems to flexible, learning-based models capable of continuous improvement. By evaluating RL algorithms in realistic, high-fidelity driving scenarios, this project contributes to the development of autonomous systems that are not just operational, but robust, scalable, and safe for large-scale adoption.\n",
    "\n",
    "**Refrences** \n",
    "* National Highway Traffic Safety Administration (NHTSA). (2015). Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey.\n",
    "* Tesla AI Team. (2021). Tesla Autonomy Day / AI Day. https://www.tesla.com/AI\n",
    "* Waymo Safety Report. (2020). https://waymo.com/safety\n",
    "* Aradi, S. (2020). Survey of deep reinforcement learning for motion planning of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 22(6), 3023-3035."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Related Work**\n",
    "There has been extensive work applying reinforcement learning (RL) to autonomous vehicles. Industry leaders such as Tesla and Waymo have pioneered systems capable of making real-time driving decisions using advanced machine learning. Tesla’s Autopilot leverages deep neural networks to detect lane lines, classify objects, and perform lane-change maneuvers in real time (Tesla AI Team, 2021). Waymo has taken a modular approach, combining deep learning and imitation learning to build fully autonomous vehicles that navigate complex urban environments (Waymo Team, 2020). Both companies rely on deep learning models for core control functions like steering, braking, and obstacle avoidance.\n",
    "\n",
    "In academic research, simulation environments are widely used to train and evaluate RL-based driving agents. One such environment is highway-env, a lightweight and highly configurable simulation tool designed for studying high-level decision-making in highway scenarios. Unlike photorealistic simulators like CARLA, highway-env focuses on lane-based traffic behavior, allowing for faster training and experimentation with various RL algorithms such as Deep Q-Networks (DQN) (Mnih et al., 2015) and Proximal Policy Optimization (PPO) (Schulman et al., 2017). These methods have been used to train agents to change lanes, avoid collisions, and maintain safe distances in dynamically changing traffic conditions.\n",
    "\n",
    "Additional inspiration comes from DeepMind’s work, which explores how reinforcement learning can be used to train robots through simulated environments (Silver et al., 2016). Although their work focuses more broadly on robotics and game-playing agents, the underlying techniques—especially regarding generalization and environment interaction—are highly relevant to the autonomous driving domain.\n",
    "\n",
    "Our project builds on these efforts by using RL algorithms within a simulation environment to investigate how well different techniques handle real-world driving challenges such as unpredictable traffic, pedestrians, and sudden obstacles. The goal is to understand which algorithms generalize best, and what limitations may arise when transferring policies from simulation to reality.\n",
    "\n",
    "**References**\n",
    "\n",
    "- Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. *Conference on Robot Learning (CoRL)*.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.\n",
    "- Silver, D., Huang, A., Maddison, C. J., et al. (2016). Mastering the game of Go with deep neural networks and tree search. *Nature*.\n",
    "- Tesla AI Team. (2021). *Tesla AI Day 2021*. [https://www.tesla.com/AI](https://www.tesla.com/AI)\n",
    "- Waymo Team. (2020). *On the Road to Fully Autonomous Driving*. [https://waymo.com/research](https://waymo.com/research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Main Research Questions**\n",
    "This research investigates how reinforcement learning (RL) can enhance autonomous vehicle (AV) decision-making in complex, real-world environments. The following questions guide our inquiry:\n",
    "\n",
    "1. **How can reinforcement learning improve decision-making for autonomous vehicles in high-speed, multi-agent highway environments?**  \n",
    "   In highway driving, autonomous vehicles must make rapid decisions while interacting with other moving vehicles—merging, overtaking, maintaining safe distances, and responding to variable traffic conditions. We aim to assess whether RL-based control systems can enable AVs to make safer, more efficient decisions compared to fixed-policy baselines in simulated highway scenarios using the highway-env environment.\n",
    "\n",
    "2. **Which RL algorithms are most effective for training autonomous vehicles to navigate traffic, avoid collisions, and adapt to changing environments?**  \n",
    "   To address this question, we will compare several widely studied RL algorithms—Q-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO)—in the highway-env. These algorithms differ in their learning dynamics, representational capacity, and suitability for discrete versus continuous action spaces. By evaluating their performance across various tasks (e.g., lane-following, obstacle avoidance, dynamic rerouting), we seek to identify which approach best balances learning efficiency, stability, and safety.\n",
    "\n",
    "3. **How well do RL agents trained in one driving scenario generalize to new, unseen highway conditions?**  \n",
    "   High performance in simulation does not guarantee successful deployment in the physical world. This question examines the “reality gap”: the extent to which learned policies generalize from synthetic to real-world domains. We will analyze agent behavior under unseen conditions (e.g., increased traffic density, occluded objects, new traffic rules, aggressive driver behavior) to evaluate generalization, robustness, and the potential need for domain adaptation techniques.\n",
    "\n",
    "4. **What are the key limitations of using reinforcement learning in autonomous driving, and how can they be mitigated?**  \n",
    "   Despite its potential, RL presents unique challenges in the context of AVs, including sample inefficiency, safety during exploration, reward design complexity, and ethical ambiguity in edge-case scenarios (e.g., unavoidable collisions). Our work aims to surface these limitations and propose possible strategies for addressing them, such as incorporating safety constraints, curriculum learning, or hybrid models that combine RL with rule-based oversight.\n",
    "\n",
    "Together, these questions structure our investigation into how reinforcement learning can serve as a foundation for more intelligent, adaptable, and trustworthy autonomous driving systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5 Our Hypothesis**\n",
    "We hypothesize that **reinforcement learning (RL) algorithms can significantly improve the real-time decision-making capabilities of autonomous vehicles by enabling agents to learn from environmental interactions and iteratively refine their behavior over time.** \n",
    "\n",
    "Specifically, we anticipate the following:\n",
    "\n",
    "- **Deep Q-Networks (DQN)** will be effective for discrete, task-specific behaviors commonly seen in highway-env scenarios, such as lane keeping, overtaking, and collision avoidance. DQN’s ability to approximate Q-values in high-dimensional observation spaces allows it to optimize long-term safety and efficiency in structured, rule-based traffic environments.\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)** will demonstrate superior performance in more complex or adaptive driving tasks, particularly when extended to environments with continuous action components (e.g., speed modulation and strategic decision-making). PPO’s stability and robustness across policy updates make it well-suited for learning consistent, safe behavior under dynamic multi-agent conditions.\n",
    "\n",
    "We expect that RL-trained agents will outperform traditional rule-based baselines in key metrics such as collision rate, lane discipline, and travel time efficiency within highway-env. We also anticipate that these agents will display adaptive behavior under variations in traffic density, aggressiveness of other vehicles, and environmental configurations. However, we also recognize that challenges will persist in generalizing learned policies beyond their training configurations. While highway-env supports controlled experimentation, transferring behavior to real-world driving systems would require addressing gaps in sensor fidelity, environment complexity, and unpredictable human behavior.\n",
    "\n",
    "Overall, we hypothesize that while reinforcement learning offers a powerful framework for autonomous driving, its practical deployment will depend on solving key challenges related to scalability, reliability, and transferability across domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Methodology & Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- You have a great deal of free rein to decide what analyses you should use and therefore you will be graded on the appropriateness of the methods chosen. \n",
    "- Argue for a few analyses that can help to answer your research question(s). \n",
    "- You should argue for more than one kind of analysis to help answer your research questions. \n",
    "- Try to make this the first draft of your methodology. \n",
    "- This will eventually turn into 1-2 slides on methodology in your final presentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology & Analysis\n",
    "To investigate how reinforcement learning (RL) can improve real-time decision-making in autonomous vehicles, we will implement and compare three RL algorithms—Q-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO)—in a controlled simulation environment. Our methodology consists of three components: simulation design, algorithm implementation, and multi-faceted evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simulation Environment\n",
    "\n",
    "All experiments will be conducted using CARLA, an open-source, high-fidelity urban driving simulator designed for autonomous vehicle research. CARLA offers configurable weather, lighting, traffic density, pedestrian movement, and road layouts, enabling us to evaluate learning agents under diverse and unpredictable driving conditions. Scenarios will include both structured tasks (e.g., lane following, navigation to a target location) and unstructured ones (e.g., obstacle avoidance in dynamic traffic).\n",
    "\n",
    "By using simulation, we ensure a reproducible, scalable, and risk-free setting in which to train agents over many episodes, including scenarios that would be unsafe or infeasible to test in real vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reinforcement Learning Algorithms\n",
    "\n",
    "We will implement the following RL algorithms for comparative analysis:\n",
    "\n",
    "- **Q-Learning**: A tabular method suitable for discrete state and action spaces. While limited in scalability, it serves as a useful baseline to establish the difficulty of tasks and explore the constraints of simple value-based learning.\n",
    "  \n",
    "- **Deep Q-Networks (DQN)**: An extension of Q-learning that utilizes deep neural networks to approximate Q-values in high-dimensional state spaces. DQN is widely used in control problems and has shown strong performance in tasks involving perception and sequential decision-making.\n",
    "  \n",
    "- **Proximal Policy Optimization (PPO)**: A state-of-the-art policy gradient method known for its sample efficiency and training stability, particularly in continuous action environments such as vehicle control. PPO is expected to perform well in tasks requiring fine-grained control over steering, acceleration, and braking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Experimental Tasks\n",
    "\n",
    "To assess the generalizability and robustness of each algorithm, agents will be evaluated on a progression of tasks:\n",
    "\n",
    "1. **Basic Lane-Following**: Tests stability and path adherence under ideal conditions.\n",
    "2. **Dynamic Obstacle Avoidance**: Introduces moving vehicles and pedestrians.\n",
    "3. **Urban Navigation with Traffic Rules**: Requires the agent to follow stop signs, traffic lights, and make turn decisions.\n",
    "4. **Environmental Perturbation**: Evaluates policy robustness under rain, fog, and nighttime conditions.\n",
    "\n",
    "Each agent will be trained on a subset of tasks and tested on both seen and unseen conditions to measure generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation Metrics\n",
    "\n",
    "Our analysis will include both **quantitative** and **qualitative** metrics:\n",
    "\n",
    "- **Safety**: Number of collisions, near-misses, or traffic violations per episode.\n",
    "- **Efficiency**: Time-to-completion and distance traveled.\n",
    "- **Reward Accumulation**: Cumulative reward across training and evaluation episodes.\n",
    "- **Policy Robustness**: Drop in performance when transferred to new or perturbed environments.\n",
    "- **Convergence Behavior**: Training stability and learning curves across episodes.\n",
    "\n",
    "These metrics will allow us to compare not only raw performance but also the reliability and adaptability of each RL approach under varying conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Analysis Strategy\n",
    "\n",
    "We will conduct both **intra-algorithm analysis** (comparing performance of each algorithm across tasks and conditions) and **inter-algorithm comparison** (benchmarking algorithms against one another). Additionally, visualizations such as reward curves, policy heatmaps, and behavioral traces will be used to illustrate differences in decision-making behavior.\n",
    "\n",
    "Qualitative insights (e.g., agent hesitation at intersections or failure under occlusion) will also be recorded through visual inspection and frame-by-frame analysis of edge cases. This will help identify specific limitations in the learned policies and suggest directions for future improvement (e.g., curriculum learning, hybrid models, or safety constraints).\n",
    "\n",
    "Together, these methods offer a comprehensive approach to evaluating the practical potential of reinforcement learning for autonomous vehicle decision-making in complex and variable environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Visualizations & Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- Perform one part of the analysis to help answer one (or more) research question(s). \n",
    "- Create visualizations that will help to provide evidence. \n",
    "- Discuss the results and how they provide evidence for answering the research questions. \n",
    "- Try to make this a first draft of one part of the results for the project.\n",
    "- Try to have at least one visualization that you plan to use as a figure in the final presentation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
