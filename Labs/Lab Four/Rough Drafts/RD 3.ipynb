{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4: First Draft of Final Project**\n",
    "    - Salissa Hernandez\n",
    "    - Juan Carlos Dominguez\n",
    "    - Leonardo Piedrahita\n",
    "    - Brice Danvide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is meant to be the first draft of motivation, methods, and one result for the final project of the class. In this lab, you will select a topic to investigate and perform an analysis to understand if this topic is of sufficient interest to use as a final project. You will be writing down many of these aspects in preparation for a final project presentation. \n",
    "\n",
    "For the final class project, you should investigate a topic from the course of your choosing. This topic can be related to anything we have discussed. For instance, you might choose to investigate a new implementation of Stable Diffusion training with an alternative latent space function (or new form of cross attention). Please do not let this example bias your choosing of a project--the topic you choose to investigate does not need to be a new algorithmic approach. It could be a new application of an algorithm we discussed. For instance, you might choose to use multi-task modeling for assessing robotic surgery. Or you might choose a new topic in ethical considerations of models. The only requirement is that the idea be something that creates new knowledge in the world and is somehow related to the vast number of topics we discussed in the class. \n",
    "\n",
    "Any topic can be chosen from ethical machine learning, convolutional visualization, data generation (with VAE's or GANs), multi-task or multi-modal architectures, stable diffusion, style transfer, or reinforcement learning. \n",
    "\n",
    "This lab will help you to ensure that the topic is appropriate for a final project in the course in terms of scope (not so easy that it might considered trivial, but not so hard that it might be considered a full blown dissertation). The right aim of scope should be such that you are investigating the initial analysis of a research topic--but there would still be much work to do for a full research publication. If you are unsure if your topic is appropriate, please contact the instructor for feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objective**\n",
    "For our final project, we explore the application of **Reinforcement Learning** (RL) to the domain of **autonomous vehicles**, focusing on how RL can improve safety and decision-making processes in dynamic driving environments. Our analysis involves using a **simulated highway environment** (via highway-env) to train an RL agent to perform basic tasks like navigation, obstacle avoidance, and interaction with other vehicles. By implementing and testing different RL algorithms, we seek to understand their effectiveness in making real-time driving decisions and to analyze how these algorithms can be adapted for real-world applications.\n",
    "\n",
    "Our **primary objective** is to determine how RL can be used to teach an autonomous vehicle to navigate safely and efficiently within a simulated environment, and to assess the performance of different algorithms in handling complex driving scenarios. Additionally, we will explore the broader implications of RL in autonomous driving, including safety, efficiency, and ethics, with the goal of providing insights into potential challenges when deploying RL-based systems in real-wrold autonomous vehicles. \n",
    "\n",
    "### **Terminology Used in This Project**\n",
    "To ensure clarity and understanding in our analysis, we defined the following key terms use throughout our project:\n",
    "- Reinforcement Learning (RL): A type of machine learning where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties based on its actions (Medium)\n",
    "- Agent: The autonomous vehicle in this scenario, which learns to navigate and make driving decisions based on the environment and rewards. (OpenAI Spinning Up)\n",
    "- Environment: The simulated world in which the agent operates, which includes roads, traffic signs, other vehicles, pedestrians, and obstacles (OpenAI Spinning Up)\n",
    "- State: A specific configuration of the environment, such as the position and speed of the vehicle, the distance to obstacles, or the presence of other cars. (OpenAI Spinning Up)\n",
    "- Action: The decision made by the agent at a given state, such as steering left, speeding up, or braking. (OpenAI Spinning Up)\n",
    "- Reward: A value assigned to the agent's actions, which helps it learn which behaviors are desirable (e.g., avoiding collisions or following traffic laws). (OpenAI Spinning Up)\n",
    "- Q-Learning / Deep Q-Networks (DQN): RL algorithms used to estimate the expected future rewards for each action, helping the agent make optimal decisions in a given state. (HuggingFace)\n",
    "- Proximal Policy Optimization (PPO): A state-of-the-art RL algorithm used for training in environments with large action spaces, such as controlling a vehicle's speed and direction. (HuggingFace)\n",
    "\n",
    "Sources:\n",
    "- Medium: https://medium.com/%40gurkanc/deep-reinforcement-learning-agents-algorithms-and-strategies-a-practical-game-scenario-a412428ae0e0\n",
    "- OpenAI Spinning Up - Intro to RL: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
    "- HuggingFace: https://huggingface.co/learn/deep-rl-course/en/unit3/from-q-to-dqn\n",
    "- HuggingFace: https://huggingface.co/blog/deep-rl-ppo\n",
    "\n",
    "References:\n",
    "- https://github.com/Farama-Foundation/HighwayEnv?tab=readme-ov-file\n",
    "- https://highway-env.farama.org/quickstart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Motivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- Motivate the need for the research project. \n",
    "- Why is this investigation important? \n",
    "- What related work are you building from? \n",
    "- What are the main research question(s)? \n",
    "- What is your hypothesis for what will happen? \n",
    "- This section should be something that can be converted into two or three slides for the final presentation. \n",
    "- You should write down the motivations and related work that will be presented to the instructor later on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Motivation for Our Research Project**\n",
    "Autonomous vehicles (AVs) represent one of the most transformative innovations in the modern transportation industry. While the technological advancements are promising from current projects like Waymo, the ability of AVs to make safe, efficient, and ethical driving decisions in real-time remains a critical challenge. AVs operate in highly dynamic environments with unpredictable conditions such as varying traffic patterns, road conditions, weather, and interactions with pedestrians and other vehicles. Reinforcement learning offers an opportunity to optimize the decision-making process through experience-based learning.\n",
    "\n",
    "The **motivation behind this project is to investigate how RL can improve AVs‚Äô performance in terms of safety, efficiency, and adaptability**. By utilizing RL algorithms, we hope to enhance the vehicle‚Äôs ability to learn from its environment and make intelligent decisions, such as when to accelerate, brake, and avoid obstacles. These algorithms can also learn from different driving scenarios, enabling the vehicle to perform better in diverse real-world conditions, potentially reducing accidents and improving traffic flow.\n",
    "\n",
    "The need for autonomous vehicles stems from a broader societal context. Human error accounts for the overwhelming majority of traffic accidents, and autonomous systems offer the potential to significantly reduce these numbers by eliminating distractions, fatigue, and impaired judgment. Moreover, AVs can provide mobility for individuals who are unable to drive, increase transportation efficiency through intelligent routing and coordination, and contribute to environmental sustainability by reducing emissions through smoother, optimized driving. As we move toward a future where AVs are more widely deployed, it becomes increasingly important to ensure these systems are capable of making real-time, intelligent decisions in complex environments. This project contributes to that goal by exploring reinforcement learning as a scalable, adaptable framework for safe and intelligent autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Why is this investigation important?**\n",
    "The importance of this research lies in addressing two of the most pressing challenges in the field of autonomous driving: safety and real-time decision-making in uncertain environments. While modern autonomous vehicle (AV) systems‚Äîsuch as Tesla‚Äôs Autopilot, Waymo‚Äôs Driver, and Cruise‚Äôs Origin platform‚Äîhave demonstrated considerable progress, they remain limited in their ability to respond effectively to emergent, high-stakes scenarios. These include sudden pedestrian crossings, dynamic lane changes, traffic signal malfunctions, construction detours, and adverse weather conditions like heavy rain or fog. Several high-profile incidents underscore this limitation.\n",
    "\n",
    "In 2023, Cruise paused all autonomous operations across the United States after an incident in San Francisco where one of its vehicles failed to yield appropriately and dragged a pedestrian following a collision caused by another car. Similarly, Tesla‚Äôs Autopilot and Full Self-Driving (FSD) systems have faced multiple investigations by the National Highway Traffic Safety Administration (NHTSA) due to collisions with stationary emergency vehicles and phantom braking events. Even Waymo, considered one of the most technically mature systems, has encountered issues with overly cautious behavior‚Äîsuch as freezing in intersections or failing to merge‚Äîcausing confusion for human drivers and traffic flow disruptions.\n",
    "\n",
    "These failures are rooted in the rigidity of current AV decision-making frameworks, which are largely deterministic or supervised-learning-based. Such systems rely on predefined rules or behavior learned from labeled datasets, limiting their ability to generalize to rare or novel situations. As a result, they are often brittle, overly conservative, or prone to misclassification under conditions that diverge from their training data.\n",
    "\n",
    "Reinforcement learning (RL) offers a compelling alternative by enabling agents to learn optimal policies through interaction and feedback. Rather than relying solely on human-designed heuristics, RL-based systems iteratively refine their behavior by observing the long-term consequences of their actions. This allows the AV to learn nuanced driving behaviors‚Äîsuch as yielding to aggressive drivers, dynamically rerouting around construction, or adjusting speed in low-visibility weather‚Äîthat would be difficult to hand-code or annotate in large-scale datasets.\n",
    "\n",
    "The relevance of this investigation is further underscored by the increasing push for AV deployment on public roads, where safety is a primary concern. According to the National Highway Traffic Safety Administration (NHTSA), over 90% of vehicular accidents are attributable to human error. As AVs are introduced into mixed traffic ecosystems, their ability to make adaptive, ethical, and reliable decisions in real time becomes critical‚Äînot only for reducing accidents but also for gaining public trust and regulatory approval.\n",
    "\n",
    "This research aligns with the broader shift in the AV industry from rigid, rule-driven systems to flexible, learning-based models capable of continuous improvement. By evaluating RL algorithms in realistic, high-fidelity driving scenarios, this project contributes to the development of autonomous systems that are not just operational, but robust, scalable, and safe for large-scale adoption.\n",
    "\n",
    "**Refrences** \n",
    "* National Highway Traffic Safety Administration (NHTSA). (2015). Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey.\n",
    "* Tesla AI Team. (2021). Tesla Autonomy Day / AI Day. https://www.tesla.com/AI\n",
    "* Waymo Safety Report. (2020). https://waymo.com/safety\n",
    "* Aradi, S. (2020). Survey of deep reinforcement learning for motion planning of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 22(6), 3023-3035."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Related Work**\n",
    "There has been extensive work applying reinforcement learning (RL) to autonomous vehicles. Industry leaders such as Tesla and Waymo have pioneered systems capable of making real-time driving decisions using advanced machine learning. Tesla‚Äôs Autopilot leverages deep neural networks to detect lane lines, classify objects, and perform lane-change maneuvers in real time (Tesla AI Team, 2021). Waymo has taken a modular approach, combining deep learning and imitation learning to build fully autonomous vehicles that navigate complex urban environments (Waymo Team, 2020). Both companies rely on deep learning models for core control functions like steering, braking, and obstacle avoidance.\n",
    "\n",
    "In academic research, simulation environments are widely used to train and evaluate RL-based driving agents. One such environment is highway-env, a lightweight and highly configurable simulation tool designed for studying high-level decision-making in highway scenarios. Unlike photorealistic simulators like CARLA, highway-env focuses on lane-based traffic behavior, allowing for faster training and experimentation with various RL algorithms such as Deep Q-Networks (DQN) (Mnih et al., 2015) and Proximal Policy Optimization (PPO) (Schulman et al., 2017). These methods have been used to train agents to change lanes, avoid collisions, and maintain safe distances in dynamically changing traffic conditions.\n",
    "\n",
    "Additional inspiration comes from DeepMind‚Äôs work, which explores how reinforcement learning can be used to train robots through simulated environments (Silver et al., 2016). Although their work focuses more broadly on robotics and game-playing agents, the underlying techniques‚Äîespecially regarding generalization and environment interaction‚Äîare highly relevant to the autonomous driving domain.\n",
    "\n",
    "Our project builds on these efforts by using RL algorithms within a simulation environment to investigate how well different techniques handle real-world driving challenges such as unpredictable traffic, pedestrians, and sudden obstacles. The goal is to understand which algorithms generalize best, and what limitations may arise when transferring policies from simulation to reality.\n",
    "\n",
    "**References**\n",
    "\n",
    "- Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. *Conference on Robot Learning (CoRL)*.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.\n",
    "- Silver, D., Huang, A., Maddison, C. J., et al. (2016). Mastering the game of Go with deep neural networks and tree search. *Nature*.\n",
    "- Tesla AI Team. (2021). *Tesla AI Day 2021*. [https://www.tesla.com/AI](https://www.tesla.com/AI)\n",
    "- Waymo Team. (2020). *On the Road to Fully Autonomous Driving*. [https://waymo.com/research](https://waymo.com/research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Main Research Questions**\n",
    "This research investigates how reinforcement learning (RL) can enhance autonomous vehicle (AV) decision-making in complex, real-world environments. The following questions guide our inquiry:\n",
    "\n",
    "1. **How can reinforcement learning improve decision-making for autonomous vehicles in high-speed, multi-agent highway environments?**  \n",
    "   In highway driving, autonomous vehicles must make rapid decisions while interacting with other moving vehicles‚Äîmerging, overtaking, maintaining safe distances, and responding to variable traffic conditions. We aim to assess whether RL-based control systems can enable AVs to make safer, more efficient decisions compared to fixed-policy baselines in simulated highway scenarios using the highway-env environment.\n",
    "\n",
    "2. **Which RL algorithms are most effective for training autonomous vehicles to navigate traffic, avoid collisions, and adapt to changing environments?**  \n",
    "   To address this question, we will compare several widely studied RL algorithms‚ÄîQ-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO)‚Äîin the highway-env. These algorithms differ in their learning dynamics, representational capacity, and suitability for discrete versus continuous action spaces. By evaluating their performance across various tasks (e.g., lane-following, obstacle avoidance, dynamic rerouting), we seek to identify which approach best balances learning efficiency, stability, and safety.\n",
    "\n",
    "3. **How well do RL agents trained in one driving scenario generalize to new, unseen highway conditions?**  \n",
    "   High performance in simulation does not guarantee successful deployment in the physical world. This question examines the ‚Äúreality gap‚Äù: the extent to which learned policies generalize from synthetic to real-world domains. We will analyze agent behavior under unseen conditions (e.g., increased traffic density, occluded objects, new traffic rules, aggressive driver behavior) to evaluate generalization, robustness, and the potential need for domain adaptation techniques.\n",
    "\n",
    "4. **What are the key limitations of using reinforcement learning in autonomous driving, and how can they be mitigated?**  \n",
    "   Despite its potential, RL presents unique challenges in the context of AVs, including sample inefficiency, safety during exploration, reward design complexity, and ethical ambiguity in edge-case scenarios (e.g., unavoidable collisions). Our work aims to surface these limitations and propose possible strategies for addressing them, such as incorporating safety constraints, curriculum learning, or hybrid models that combine RL with rule-based oversight.\n",
    "\n",
    "Together, these questions structure our investigation into how reinforcement learning can serve as a foundation for more intelligent, adaptable, and trustworthy autonomous driving systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5 Our Hypothesis**\n",
    "We hypothesize that **reinforcement learning (RL) algorithms can significantly improve the real-time decision-making capabilities of autonomous vehicles by enabling agents to learn from environmental interactions and iteratively refine their behavior over time.** \n",
    "\n",
    "Specifically, we anticipate the following:\n",
    "\n",
    "- **Deep Q-Networks (DQN)** will be effective for discrete, task-specific behaviors commonly seen in highway-env scenarios, such as lane keeping, overtaking, and collision avoidance. DQN‚Äôs ability to approximate Q-values in high-dimensional observation spaces allows it to optimize long-term safety and efficiency in structured, rule-based traffic environments.\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)** will demonstrate superior performance in more complex or adaptive driving tasks, particularly when extended to environments with continuous action components (e.g., speed modulation and strategic decision-making). PPO‚Äôs stability and robustness across policy updates make it well-suited for learning consistent, safe behavior under dynamic multi-agent conditions.\n",
    "\n",
    "We expect that RL-trained agents will outperform traditional rule-based baselines in key metrics such as collision rate, lane discipline, and travel time efficiency within highway-env. We also anticipate that these agents will display adaptive behavior under variations in traffic density, aggressiveness of other vehicles, and environmental configurations. However, we also recognize that challenges will persist in generalizing learned policies beyond their training configurations. While highway-env supports controlled experimentation, transferring behavior to real-world driving systems would require addressing gaps in sensor fidelity, environment complexity, and unpredictable human behavior.\n",
    "\n",
    "Overall, we hypothesize that while reinforcement learning offers a powerful framework for autonomous driving, its practical deployment will depend on solving key challenges related to scalability, reliability, and transferability across domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Methodology & Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- You have a great deal of free rein to decide what analyses you should use and therefore you will be graded on the appropriateness of the methods chosen. \n",
    "- Argue for a few analyses that can help to answer your research question(s). \n",
    "- You should argue for more than one kind of analysis to help answer your research questions. \n",
    "- Try to make this the first draft of your methodology. \n",
    "- This will eventually turn into 1-2 slides on methodology in your final presentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Methodology & Analysis**\n",
    "To investigate how reinforcement learning (RL) can enhance autonomous vehicle decision-making, we will conduct a series of experiments using highway-env, a lightweight, configurable simulation environment designed specifically for high-speed, multi-agent driving scenarios. This environment supports a wide range of traffic configurations, driver behaviors, and lane-based interactions, making it ideal for controlled yet realistic testing of RL agents. Rather than relying on high-fidelity graphics or physics, highway-env focuses on the dynamics of strategic decision-making‚Äîsuch as lane changes, overtaking, and safe distance maintenance‚Äîenabling rapid prototyping and iteration.\n",
    "\n",
    "We will train agents using three reinforcement learning algorithms: Q-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO). Q-Learning serves as a classical baseline that provides insight into the performance of simpler tabular approaches in structured driving tasks. DQN offers a deep learning-based alternative that can handle high-dimensional observations and learn optimal policies for discrete action spaces. PPO, a more advanced on-policy algorithm, is known for its stability and sample efficiency and is particularly well-suited for dynamic, multi-agent environments where continuous decision-making and policy updates are required.\n",
    "\n",
    "To answer our first research question‚Äîhow reinforcement learning can improve decision-making in highway environments‚Äîwe will simulate common driving challenges such as dense traffic, aggressive driver behaviors, lane merging, and dynamic speed changes. RL agents will be trained to maximize cumulative reward functions designed to reinforce safe, efficient, and rule-abiding behavior. These reward functions will include incentives for lane discipline, collision avoidance, maintaining appropriate speed, and reaching destinations quickly. The improvement in agent behavior over time will be analyzed through training curves, reduced collision rates, and increased travel efficiency, allowing us to quantify how decision-making evolves through learning.\n",
    "\n",
    "To address the second research question, which concerns the effectiveness of different RL algorithms, we will compare Q-Learning, DQN, and PPO under identical training conditions. Each agent will be evaluated based on performance metrics including training stability, convergence speed, collision rate, and lane violation frequency. We will also evaluate how well each policy generalizes to test scenarios that were not seen during training. This comparative analysis will help determine which algorithm offers the best trade-off between learning efficiency, policy robustness, and real-time safety.\n",
    "\n",
    "Our third research question investigates how well trained agents generalize to new and unseen highway conditions. For this, we will systematically modify environment variables such as traffic density, aggressiveness of surrounding vehicles, and initial spawn positions. Agents will be tested in these altered settings without further training to evaluate the robustness and flexibility of the learned policies. Performance will be assessed through changes in success rate, behavioral drift, and stability across variations. This generalization analysis will help reveal the strengths and limitations of each approach when faced with real-world unpredictability.\n",
    "\n",
    "To explore the fourth research question, which concerns the challenges of using RL in autonomous driving, we will perform ablation studies and stress testing. These will involve intentionally introducing disruptions such as sudden lane blockages, incomplete sensory input, and irregular vehicle behaviors. We will examine how resilient each agent is to these edge cases and identify specific failure modes or limitations of the algorithms. In addition, we will analyze the impact of reward shaping on learned behavior, particularly how it influences ethical and safety-critical decisions. For instance, penalties for abrupt braking or red-light violations will be used to encourage alignment with real-world norms. If needed, we will also experiment with hybrid approaches that combine rule-based elements or safety layers into the RL framework to improve robustness.\n",
    "\n",
    "Throughout the project, we will log and analyze a variety of data including learning curves, action distributions, trajectory visualizations, and video rollouts. This combination of algorithm benchmarking, generalization testing, and stress analysis will provide a comprehensive foundation for understanding the strengths, limitations, and practical potential of reinforcement learning for autonomous highway driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Environment Setup**\n",
    "All experiments will be conducted using highway-env, a Python-based simulation environment designed for testing and training autonomous driving agents in highway scenarios. This environment provides a simplified yet effective abstraction of real-world driving, enabling the simulation of multi-lane roads, interacting vehicles, varying traffic densities, and driver behaviors. It supports different configurations such as lane numbers, vehicle acceleration dynamics, and traffic flow, allowing us to construct diverse training and testing conditions that reflect the complexities of high-speed highway driving.\n",
    "\n",
    "By using highway-env, we are able to simulate both structured driving tasks, such as lane keeping and reaching a destination, and more dynamic challenges like overtaking slower vehicles, avoiding collisions, and responding to sudden changes in traffic behavior. The environment allows fine-grained control over simulation parameters and provides real-time visual feedback, making it well-suited for iterative reinforcement learning development. Additionally, its lightweight and efficient design allows agents to train over thousands of episodes without the computational burden of high-resolution graphics or physical modeling, thus supporting scalable experimentation in a safe, controlled, and repeatable setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.3 Reinforcement Learning Algorithms**\n",
    "We will implement the following RL algorithms for comparative analysis:\n",
    "\n",
    "- **Q-Learning**: A tabular method suitable for discrete state and action spaces. While limited in scalability, it serves as a useful baseline to establish the difficulty of tasks and explore the constraints of simple value-based learning.\n",
    "  \n",
    "- **Deep Q-Networks (DQN)**: An extension of Q-learning that utilizes deep neural networks to approximate Q-values in high-dimensional state spaces. DQN is widely used in control problems and has shown strong performance in tasks involving perception and sequential decision-making.\n",
    "  \n",
    "- **Proximal Policy Optimization (PPO)**: A state-of-the-art policy gradient method known for its sample efficiency and training stability, particularly in continuous action environments such as vehicle control. PPO is expected to perform well in tasks requiring fine-grained control over steering, acceleration, and braking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.4 Experimental Tasks**\n",
    "To assess the generalizability and robustness of each algorithm, agents will be evaluated on a progression of tasks:\n",
    "\n",
    "1. **Basic Lane-Following**: Tests stability and path adherence under ideal conditions.\n",
    "2. **Dynamic Vehicle Interaction**: Introduces multi-agent scenarios with varying vehicle speeds and densities, requiring safe merging, overtaking, and adaptive responses.\n",
    "3. **Traffic Rule Compliance**: Evaluates adherence to basic driving norms such as lane discipline, safe following distances, and yielding behavior.\n",
    "4. **Environmental Perturbation**: Evaluates policy robustness under reduced visibility, aggressive drivers, and modified road layouts.\n",
    "\n",
    "Each agent will be trained on a subset of tasks and tested on both seen and unseen conditions to measure generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.5 Evaluation Metrics**\n",
    "Our analysis will include both **quantitative** and **qualitative** metrics:\n",
    "\n",
    "- **Safety**: Number of collisions, near-misses, or traffic violations per episode.\n",
    "- **Efficiency**: Time-to-completion and distance traveled.\n",
    "- **Reward Accumulation**: Cumulative reward across training and evaluation episodes.\n",
    "- **Policy Robustness**: Drop in performance when transferred to new or perturbed environments.\n",
    "- **Convergence Behavior**: Training stability and learning curves across episodes.\n",
    "\n",
    "These metrics will allow us to compare not only raw performance but also the reliability and adaptability of each RL approach under varying conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.6 Analysis Strategy**\n",
    "We will conduct both **intra-algorithm analysis** (comparing performance of each algorithm across tasks and conditions) and **inter-algorithm comparison** (benchmarking algorithms against one another). Additionally, visualizations such as reward curves, policy heatmaps, and behavioral traces will be used to illustrate differences in decision-making behavior.\n",
    "\n",
    "Qualitative insights (e.g., agent hesitation at intersections or failure under occlusion) will also be recorded through visual inspection and frame-by-frame analysis of edge cases. This will help identify specific limitations in the learned policies and suggest directions for future improvement (e.g., curriculum learning, hybrid models, or safety constraints).\n",
    "\n",
    "Together, these methods offer a comprehensive approach to evaluating the practical potential of reinforcement learning for autonomous vehicle decision-making in complex and variable environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Visualizations & Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- Perform one part of the analysis to help answer one (or more) research question(s). \n",
    "- Create visualizations that will help to provide evidence. \n",
    "- Discuss the results and how they provide evidence for answering the research questions. \n",
    "- Try to make this a first draft of one part of the results for the project.\n",
    "- Try to have at least one visualization that you plan to use as a figure in the final presentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.1 Analysis of Our Research Question: How can reinforcement learning improve decision-making for autonomous vehicles in high-speed, multi-agent highway environments?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modules & Libraries \n",
    "# import gymnasium\n",
    "# import gym\n",
    "# from gymnasium.wrappers import RecordVideo\n",
    "# import highway_env\n",
    "# from highway_env import utils\n",
    "# import numpy as np\n",
    "# import imageio\n",
    "# from IPython.display import Image as IPImage\n",
    "# from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from collections import deque\n",
    "# import random\n",
    "\n",
    "\n",
    "# # üö® Patch the action space to pretend it's gym.spaces.Discrete\n",
    "# import gymnasium.spaces\n",
    "# from gym import ObservationWrapper, Wrapper, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup\n",
    "# env = gymnasium.make(\"highway-v0\", render_mode=\"rgb_array\",\n",
    "#                         config={\"lanes_count\": 4, \"collision_reward\": -5,  'high_speed_reward': 0.6,\n",
    "#                                 'right_lane_reward': 0.2,         \n",
    "#                                 'lane_change_reward': -0.05,          \n",
    "#                                 'reward_speed_range': [15, 30],       \n",
    "#                                 'vehicles_count': 40,                  \n",
    "#                                 'duration': 50})\n",
    "# obs, _ = env.reset()\n",
    "\n",
    "# frames = []\n",
    "\n",
    "# # Collect frames\n",
    "# for _ in range(200):\n",
    "#     action = env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "#     obs, reward, done, truncated, info = env.step(action)\n",
    "#     frame = env.render()\n",
    "#     frames.append(frame)\n",
    "#     if done or truncated:\n",
    "#         break\n",
    "\n",
    "# env.close()\n",
    "\n",
    "# # Add 3-second pause at the end (15 frames at 5 FPS)\n",
    "# frames.extend([frames[-1]] * 15)\n",
    "\n",
    "# # Save to GIF\n",
    "# gif_path = \"trajectory.gif\"\n",
    "# imageio.mimsave(gif_path, frames, fps=5, loop=0)\n",
    "\n",
    "# # ‚úÖ Display the actual GIF correctly\n",
    "# with open(gif_path, \"rb\") as f:\n",
    "#     gif_bytes = f.read()\n",
    "\n",
    "# IPImage(data=gif_bytes, format='gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To being our analysis, a baseline was established by observing the behavior of an autonomous vehicle operating under a non-learning, IDLE policy within the highway-v0 environment. This setup involved the agent taking no meaningful action throughout the simulation, allowing for the visualization of default behavior in a dynamic, multi-agent highway scenario. The results, visualized through a trajectory GIF, highlighted the agent‚Äôs inability to adapt to its surroundings. The vehicle remained passive, often resulting in traffic congestion, collisions, or inefficient positioning relative to surrounding vehicles. This behavior clearly underscored the limitations of static decision-making in high-speed environments where responsiveness is crucial. These findings serve as an important reference point, helping to illustrate the necessity of adaptive, learning-based approaches for safe and efficient autonomous navigation. Future comparisons against reinforcement learning agents will determine the extent of improvement made possible by intelligent decision-making policies.\n",
    "\n",
    "******************* THIS NEEDS TO BE CHANGED TO SOUND LESS GPT AND ALSO BE MORE DESCRIPTIVE *****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!!! DO NOT MOVE THE FOLLOWING MODULES AWAY AS IT IS WEIRD AND MAKES THE KERNELS CRASH IF YOU KEEP THEM WITH THE OTHER MODULES ABOVE !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "# from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST: this one i set up for DQN to learn but when i ran it, it did not perform any better. runs for about 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === CUSTOM WRAPPERS (for Gymnasium compatibility) ===\n",
    "# class FlattenObservationWrapper(Wrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "#         original_shape = env.observation_space.shape\n",
    "#         self.observation_space = spaces.Box(\n",
    "#             low=-np.inf,\n",
    "#             high=np.inf,\n",
    "#             shape=(np.prod(original_shape),),\n",
    "#             dtype=np.float32\n",
    "#         )\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         return observation.flatten()\n",
    "\n",
    "#     def step(self, action):\n",
    "#         obs, reward, done, info = self.env.step(action)\n",
    "#         return self.observation(obs), reward, done, info\n",
    "\n",
    "#     def reset(self, **kwargs):\n",
    "#         obs, _ = self.env.reset(**kwargs)\n",
    "#         return self.observation(obs)\n",
    "\n",
    "# class GymnasiumToGymWrapper(Wrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "#         if isinstance(env.action_space, gymnasium.spaces.Discrete):\n",
    "#             self.action_space = spaces.Discrete(env.action_space.n)\n",
    "\n",
    "# class GymnasiumStepAPICompatibilityWrapper(Wrapper):\n",
    "#     def step(self, action):\n",
    "#         obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "#         done = terminated or truncated\n",
    "#         return obs, reward, done, info\n",
    "\n",
    "# # === ENVIRONMENT SETUP (Experimental Task 1: Lane Following) ===\n",
    "# def make_wrapped_env():\n",
    "#     env = gymnasium.make(\"highway-v0\", render_mode=\"rgb_array\",\n",
    "#                             config={\"lanes_count\": 4, \"collision_reward\": -5,  'high_speed_reward': 0.6,\n",
    "#                                     'right_lane_reward': 0.2,         \n",
    "#                                     'lane_change_reward': -0.05,          \n",
    "#                                     'reward_speed_range': [15, 30],       \n",
    "#                                     'vehicles_count': 40,                  \n",
    "#                                     'duration': 50})\n",
    "#     env = GymnasiumStepAPICompatibilityWrapper(env)\n",
    "#     env = GymnasiumToGymWrapper(env)\n",
    "#     env = FlattenObservationWrapper(env)\n",
    "#     return env\n",
    "\n",
    "# env = DummyVecEnv([make_wrapped_env])\n",
    "# print(f\"Action Space: {env.action_space}\")\n",
    "# print(f\"Observation Space: {env.observation_space}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === BASELINE (IDLE Policy) ===\n",
    "# idle_env = gymnasium.make(\"highway-v0\", render_mode=\"rgb_array\",\n",
    "#                           config={\"lanes_count\": 4, \"collision_reward\": -5,  'high_speed_reward': 0.6,\n",
    "#                                 'right_lane_reward': 0.2,         \n",
    "#                                 'lane_change_reward': -0.05,          \n",
    "#                                 'reward_speed_range': [15, 30],       \n",
    "#                                 'vehicles_count': 40,                  \n",
    "#                                 'duration': 50})\n",
    "# idle_env.reset()\n",
    "# frames = []\n",
    "\n",
    "# for _ in range(200):\n",
    "#     action = idle_env.unwrapped.action_type.actions_indexes[\"IDLE\"]\n",
    "#     obs, reward, done, truncated, _ = idle_env.step(action)\n",
    "#     frames.append(idle_env.render())\n",
    "#     if done or truncated:\n",
    "#         break\n",
    "\n",
    "# frames.extend([frames[-1]] * 15)\n",
    "# imageio.mimsave(\"idle_baseline.gif\", frames, fps=5)\n",
    "\n",
    "# with open(\"idle_baseline.gif\", \"rb\") as f:\n",
    "#     gif_bytes = f.read()\n",
    "# IPImage(data=gif_bytes, format='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Step 2: Create the DQN model ---\n",
    "# model = DQN(\n",
    "#     \"MlpPolicy\",\n",
    "#     env,\n",
    "#     policy_kwargs=dict(net_arch=[256, 256]),\n",
    "#     learning_rate=5e-4,\n",
    "#     buffer_size=50000,\n",
    "#     learning_starts=1000,\n",
    "#     batch_size=32,\n",
    "#     gamma=0.95,\n",
    "#     train_freq=4,\n",
    "#     gradient_steps=4,\n",
    "#     target_update_interval=50,\n",
    "#     verbose=1,\n",
    "#     tensorboard_log=\"highway_dqn/\",\n",
    "# )\n",
    "\n",
    "# model.learn(total_timesteps=20000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === EVALUATE TRAINED MODEL ===\n",
    "# eval_env = make_wrapped_env()\n",
    "# obs = eval_env.reset()\n",
    "# frames = []\n",
    "# rewards = []\n",
    "# collisions = 0\n",
    "\n",
    "# for _ in range(200):\n",
    "#     action, _ = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = eval_env.step(action)\n",
    "#     frame = eval_env.render()\n",
    "#     frames.append(frame)\n",
    "#     rewards.append(reward)\n",
    "#     if info.get(\"crashed\", False):  # üß† Safety Metric\n",
    "#         collisions += 1\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "# frames.extend([frames[-1]] * 15)\n",
    "# imageio.mimsave(\"dqn_trajectory.gif\", frames, fps=5)\n",
    "\n",
    "# with open(\"dqn_trajectory.gif\", \"rb\") as f:\n",
    "#     gif_bytes = f.read()\n",
    "# IPImage(data=gif_bytes, format='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === METRICS & VISUALIZATION ===\n",
    "# cumulative_reward = np.sum(rewards)\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.plot(np.cumsum(rewards), label=\"Cumulative Reward\")\n",
    "# plt.xlabel(\"Timestep\")\n",
    "# plt.ylabel(\"Reward\")\n",
    "# plt.title(\"DQN Agent Cumulative Reward Over Time\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.savefig(\"reward_curve.png\")\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Total collisions: {collisions}\")\n",
    "# print(f\"Cumulative reward: {cumulative_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING BRICE'S CODE** DOES NOT WORK FOR ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import random\n",
    "import keras\n",
    "from keras import layers\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from IPython.display import Image as IPImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = {\n",
    "    \"lanes_count\": 4,\n",
    "    \"vehicles_count\": 40,\n",
    "    \"duration\": 50,\n",
    "    \"collision_reward\": -5,\n",
    "    \"high_speed_reward\": 0.6,\n",
    "    \"right_lane_reward\": 0.2,\n",
    "    \"lane_change_reward\": -0.05,\n",
    "    \"reward_speed_range\": [15, 30]\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize environment\n",
    "env = gymnasium.make(\"highway-v0\", render_mode=\"rgb_array\", config=custom_config)\n",
    "state_size = np.prod(env.observation_space.shape)  # Flattened state size\n",
    "action_size = env.action_space.n  # Number of possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# DQN Agent Class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(self.state_size,)))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, explore=True):\n",
    "        if explore and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0])\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/60, Score: 14.126669234211779, Epsilon: 1.00\n",
      "Episode: 2/60, Score: 9.71420567329756, Epsilon: 1.00\n",
      "Episode: 3/60, Score: 7.576016026617292, Epsilon: 0.99\n",
      "Episode: 4/60, Score: 3.8731755750321284, Epsilon: 0.97\n",
      "Episode: 5/60, Score: 31.36098020595295, Epsilon: 0.81\n",
      "Episode: 6/60, Score: 18.39449944165052, Epsilon: 0.74\n",
      "Episode: 7/60, Score: 14.56860751392923, Epsilon: 0.69\n",
      "Episode: 8/60, Score: 46.2127475923143, Epsilon: 0.54\n",
      "Episode: 9/60, Score: 22.134923648547083, Epsilon: 0.48\n",
      "Episode: 10/60, Score: 5.7002053481499395, Epsilon: 0.46\n",
      "Episode: 11/60, Score: 31.32028652219161, Epsilon: 0.39\n",
      "Episode: 12/60, Score: 10.368640106565302, Epsilon: 0.37\n",
      "Episode: 13/60, Score: 19.061650411600453, Epsilon: 0.33\n",
      "Episode: 14/60, Score: 45.485472673444725, Epsilon: 0.26\n",
      "Episode: 15/60, Score: 22.534964636135694, Epsilon: 0.23\n",
      "Episode: 16/60, Score: 45.292990638312915, Epsilon: 0.18\n",
      "Episode: 17/60, Score: 37.95118836186418, Epsilon: 0.15\n",
      "Episode: 18/60, Score: 45.291114595126956, Epsilon: 0.11\n",
      "Episode: 19/60, Score: 45.5032964876896, Epsilon: 0.10\n",
      "Episode: 20/60, Score: 45.42490270375512, Epsilon: 0.10\n",
      "Episode: 21/60, Score: 45.09303849172676, Epsilon: 0.10\n",
      "Episode: 22/60, Score: 45.752416099892706, Epsilon: 0.10\n"
     ]
    }
   ],
   "source": [
    "# Initialize agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "batch_size = 32\n",
    "\n",
    "# Training loop\n",
    "episodes = 60\n",
    "scores = []\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state.flatten(), [1, state_size])\n",
    "    score = 0\n",
    "    max_steps = 200\n",
    "    for time in range(max_steps):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state.flatten(), [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    scores.append(score)\n",
    "    print(f\"Episode: {e+1}/{episodes}, Score: {score}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(scores)\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop for IDLE policy (baseline)\n",
    "env_idle = gymnasium.make(\"highway-v0\", render_mode=\"rgb_array\", config=custom_config)\n",
    "state, _ = env_idle.reset()\n",
    "state = np.reshape(state.flatten(), [1, state_size])\n",
    "\n",
    "# Collect frames during testing for IDLE policy\n",
    "frames_idle = []\n",
    "for _ in range(200):\n",
    "    action = env_idle.unwrapped.action_type.actions_indexes[\"IDLE\"]  # IDLE policy action\n",
    "    next_state, reward, done, truncated, _ = env_idle.step(action)\n",
    "    frame = env_idle.render()\n",
    "    frames_idle.append(frame)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "env_idle.close()\n",
    "\n",
    "# Save IDLE policy frames as GIF\n",
    "gif_path_idle = \"idle_baseline.gif\"\n",
    "imageio.mimsave(gif_path_idle, frames_idle, fps=5)\n",
    "\n",
    "# Display IDLE GIF first\n",
    "with open(gif_path_idle, \"rb\") as f:\n",
    "    gif_bytes = f.read()\n",
    "\n",
    "IPImage(data=gif_bytes, format='gif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop for DQN agent\n",
    "env_test = gymnasium.make(\"highway-v0\", render_mode=\"rgb_array\", config=custom_config)\n",
    "state, _ = env_test.reset()\n",
    "state = np.reshape(state.flatten(), [1, state_size])\n",
    "\n",
    "# Collect frames during testing for DQN agent\n",
    "frames_dqn = []\n",
    "for _ in range(200):\n",
    "    action = agent.act(state, explore=False)\n",
    "    next_state, reward, done, truncated, _ = env_test.step(action)\n",
    "    next_state = np.reshape(next_state.flatten(), [1, state_size])\n",
    "    state = next_state\n",
    "    frame = env_test.render()\n",
    "    frames_dqn.append(frame)\n",
    "    if done or truncated:\n",
    "        break\n",
    "\n",
    "env_test.close()\n",
    "\n",
    "# Save DQN agent's frames as GIF\n",
    "gif_path_dqn = \"dqn_trajectory.gif\"\n",
    "imageio.mimsave(gif_path_dqn, frames_dqn, fps=5)\n",
    "\n",
    "# Display DQN GIF after IDLE GIF\n",
    "with open(gif_path_dqn, \"rb\") as f:\n",
    "    gif_bytes = f.read()\n",
    "\n",
    "IPImage(data=gif_bytes, format='gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
