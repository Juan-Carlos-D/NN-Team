{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4: First Draft of Final Project**\n",
    "    - Salissa Hernandez\n",
    "    - Juan Carlos Dominguez\n",
    "    - Leonardo Piedrahita\n",
    "    - Brice Danvide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is meant to be the first draft of motivation, methods, and one result for the final project of the class. In this lab, you will select a topic to investigate and perform an analysis to understand if this topic is of sufficient interest to use as a final project. You will be writing down many of these aspects in preparation for a final project presentation. \n",
    "\n",
    "For the final class project, you should investigate a topic from the course of your choosing. This topic can be related to anything we have discussed. For instance, you might choose to investigate a new implementation of Stable Diffusion training with an alternative latent space function (or new form of cross attention). Please do not let this example bias your choosing of a project--the topic you choose to investigate does not need to be a new algorithmic approach. It could be a new application of an algorithm we discussed. For instance, you might choose to use multi-task modeling for assessing robotic surgery. Or you might choose a new topic in ethical considerations of models. The only requirement is that the idea be something that creates new knowledge in the world and is somehow related to the vast number of topics we discussed in the class. \n",
    "\n",
    "Any topic can be chosen from ethical machine learning, convolutional visualization, data generation (with VAE's or GANs), multi-task or multi-modal architectures, stable diffusion, style transfer, or reinforcement learning. \n",
    "\n",
    "This lab will help you to ensure that the topic is appropriate for a final project in the course in terms of scope (not so easy that it might considered trivial, but not so hard that it might be considered a full blown dissertation). The right aim of scope should be such that you are investigating the initial analysis of a research topic--but there would still be much work to do for a full research publication. If you are unsure if your topic is appropriate, please contact the instructor for feedback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Objective**\n",
    "For our final project, we explore the application of **Reinforcement Learning** (RL) to the domain of **autonomous vehicles**, focusing on how RL can improve safety and decision-making processes in dynamic driving environments. Our analysis involves using a **simulated driving environment** (such as CARLA) to train an RL agent to perform basic tasks like navigation, obstacle avoidance, and interaction with other vehicles. By implementing and testing different RL algorithms, we seek to understand their effectiveness in making real-time driving decisions and to analyze how these algorithms can be adapted for real-world applications.\n",
    "\n",
    "Our **primary objective** is to determine how RL can be used to teach an autonomous vehicle to navigate safely and efficiently within a simulated environment, and to assess the performance of different algorithms in handling complex driving scenarios. Additionally, we will explore the broader implications of RL in autonomous driving, including safety, efficiency, and ethics, with the goal of providing insights into potential challenges when deploying RL-based systems in real-wrold autonomous vehicles. \n",
    "\n",
    "### **Terminology Used in This Project**\n",
    "To ensure clarity and understanding in our analysis, we defined the following key terms use throughout our project:\n",
    "- Reinforcement Learning (RL): A type of machine learning where an agent learns to make decisions by interacting with an environment, receiving rewards or penalties based on its actions (Medium)\n",
    "- Agent: The autonomous vehicle in this scenario, which learns to navigate and make driving decisions based on the environment and rewards. (OpenAI Spinning Up)\n",
    "- Environment: The simulated world in which the agent operates, which includes roads, traffic signs, other vehicles, pedestrians, and obstacles (OpenAI Spinning Up)\n",
    "- State: A specific configuration of the environment, such as the position and speed of the vehicle, the distance to obstacles, or the presence of other cars. (OpenAI Spinning Up)\n",
    "- Action: The decision made by the agent at a given state, such as steering left, speeding up, or braking. (OpenAI Spinning Up)\n",
    "- Reward: A value assigned to the agent's actions, which helps it learn which behaviors are desirable (e.g., avoiding collisions or following traffic laws). (OpenAI Spinning Up)\n",
    "- Q-Learning / Deep Q-Networks (DQN): RL algorithms used to estimate the expected future rewards for each action, helping the agent make optimal decisions in a given state. (HuggingFace)\n",
    "- Proximal Policy Optimization (PPO): A state-of-the-art RL algorithm used for training in environments with large action spaces, such as controlling a vehicle's speed and direction. (HuggingFace)\n",
    "\n",
    "Sources:\n",
    "- Medium: https://medium.com/%40gurkanc/deep-reinforcement-learning-agents-algorithms-and-strategies-a-practical-game-scenario-a412428ae0e0\n",
    "- OpenAI Spinning Up - Intro to RL: https://spinningup.openai.com/en/latest/spinningup/rl_intro.html\n",
    "- HuggingFace: https://huggingface.co/learn/deep-rl-course/en/unit3/from-q-to-dqn\n",
    "- HuggingFace: https://huggingface.co/blog/deep-rl-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Motivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- Motivate the need for the research project. \n",
    "- Why is this investigation important? \n",
    "- What related work are you building from? \n",
    "- What are the main research question(s)? \n",
    "- What is your hypothesis for what will happen? \n",
    "- This section should be something that can be converted into two or three slides for the final presentation. \n",
    "- You should write down the motivations and related work that will be presented to the instructor later on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Motivation for Our Research Project**\n",
    "Autonomous vehicles (AVs) represent one of the most transformative innovations in the modern transportation industry. While the technological advancements are promising from current projects like Waymo, the ability of AVs to make safe, efficient, and ethical driving decisions in real-time remains a critical challenge. AVs operate in highly dynamic environments with unpredictable conditions such as varying traffic patterns, road conditions, weather, and interactions with pedestrians and other vehicles. Reinforcement learning offers an opportunity to optimize the decision-making process through experience-based learning.\n",
    "\n",
    "The **motivation behind this project is to investigate how RL can improve AVs’ performance in terms of safety, efficiency, and adaptability**. By utilizing RL algorithms, we hope to enhance the vehicle’s ability to learn from its environment and make intelligent decisions, such as when to accelerate, brake, and avoid obstacles. These algorithms can also learn from different driving scenarios, enabling the vehicle to perform better in diverse real-world conditions, potentially reducing accidents and improving traffic flow.\n",
    "\n",
    "The need for autonomous vehicles stems from a broader societal context. Human error accounts for the overwhelming majority of traffic accidents, and autonomous systems offer the potential to significantly reduce these numbers by eliminating distractions, fatigue, and impaired judgment. Moreover, AVs can provide mobility for individuals who are unable to drive, increase transportation efficiency through intelligent routing and coordination, and contribute to environmental sustainability by reducing emissions through smoother, optimized driving. As we move toward a future where AVs are more widely deployed, it becomes increasingly important to ensure these systems are capable of making real-time, intelligent decisions in complex environments. This project contributes to that goal by exploring reinforcement learning as a scalable, adaptable framework for safe and intelligent autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.2 Why is this investigation important?**\n",
    "The importance of this research lies in addressing two fundamental issues in autonomous driving: **safety and real-time decision-making**. Current autonomous vehicle systems, while capable, still face significant limitations in complex and unpredictable driving environments, such as handling emergent situations (e.g., sudden traffic changes, pedestrians crossing the street) or adapting to new road conditions (e.g., weather, construction zones).\n",
    "\n",
    "Furthermore, existing decision-making systems in AVs are often rule-based, relying on predefined algorithms. These can struggle to adapt to unforeseen circumstances. RL, however, allows the vehicle to learn from experience. By continuously interacting with its environment, an RL-based system can improve decision-making by understanding which actions lead to better outcomes (e.g., avoiding accidents, following traffic rules). This aligns well with the shift towards adaptive and intelligent systems in autonomous vehicles, which is necessary for widespread adoption and integration into public roads.\n",
    "\n",
    "*** MAYBE NEED MORE SPECIFIC EXAMPLES OF CURRENT AVs??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refined the above and added more examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of this research lies in addressing two of the most pressing challenges in the field of autonomous driving: safety and real-time decision-making in uncertain environments. While modern autonomous vehicle (AV) systems—such as Tesla’s Autopilot, Waymo’s Driver, and Cruise’s Origin platform—have demonstrated considerable progress, they remain limited in their ability to respond effectively to emergent, high-stakes scenarios. These include sudden pedestrian crossings, dynamic lane changes, traffic signal malfunctions, construction detours, and adverse weather conditions like heavy rain or fog. Several high-profile incidents underscore this limitation.\n",
    "\n",
    "In 2023, Cruise paused all autonomous operations across the United States after an incident in San Francisco where one of its vehicles failed to yield appropriately and dragged a pedestrian following a collision caused by another car. Similarly, Tesla’s Autopilot and Full Self-Driving (FSD) systems have faced multiple investigations by the National Highway Traffic Safety Administration (NHTSA) due to collisions with stationary emergency vehicles and phantom braking events. Even Waymo, considered one of the most technically mature systems, has encountered issues with overly cautious behavior—such as freezing in intersections or failing to merge—causing confusion for human drivers and traffic flow disruptions.\n",
    "\n",
    "These failures are rooted in the rigidity of current AV decision-making frameworks, which are largely deterministic or supervised-learning-based. Such systems rely on predefined rules or behavior learned from labeled datasets, limiting their ability to generalize to rare or novel situations. As a result, they are often brittle, overly conservative, or prone to misclassification under conditions that diverge from their training data.\n",
    "\n",
    "Reinforcement learning (RL) offers a compelling alternative by enabling agents to learn optimal policies through interaction and feedback. Rather than relying solely on human-designed heuristics, RL-based systems iteratively refine their behavior by observing the long-term consequences of their actions. This allows the AV to learn nuanced driving behaviors—such as yielding to aggressive drivers, dynamically rerouting around construction, or adjusting speed in low-visibility weather—that would be difficult to hand-code or annotate in large-scale datasets.\n",
    "\n",
    "The relevance of this investigation is further underscored by the increasing push for AV deployment on public roads, where safety is a primary concern. According to the National Highway Traffic Safety Administration (NHTSA), over 90% of vehicular accidents are attributable to human error. As AVs are introduced into mixed traffic ecosystems, their ability to make adaptive, ethical, and reliable decisions in real time becomes critical—not only for reducing accidents but also for gaining public trust and regulatory approval.\n",
    "\n",
    "This research aligns with the broader shift in the AV industry from rigid, rule-driven systems to flexible, learning-based models capable of continuous improvement. By evaluating RL algorithms in realistic, high-fidelity driving scenarios, this project contributes to the development of autonomous systems that are not just operational, but robust, scalable, and safe for large-scale adoption.\n",
    "\n",
    "**Refrences** \n",
    "* National Highway Traffic Safety Administration (NHTSA). (2015). Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey.\n",
    "* Tesla AI Team. (2021). Tesla Autonomy Day / AI Day. https://www.tesla.com/AI\n",
    "* Waymo Safety Report. (2020). https://waymo.com/safety\n",
    "* Aradi, S. (2020). Survey of deep reinforcement learning for motion planning of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 22(6), 3023-3035."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.3 Related Work**\n",
    "There has already been a lot of work done in applying reinforcement learning to autonomous vehicles. Companies like **Tesla** and **Waymo** have been leading the way in developing autonomous systems that can make real-time driving decisions using advanced machine learning techniques. Tesla’s Autopilot and Waymo’s fully autonomous cars both rely on deep learning models to control actions like steering, braking, and navigating complex environments. In the academic space, one of the most widely used tools is CARLA (Car Learning to Act), an open-source driving simulator created specifically for testing and training autonomous vehicles. Many researchers have used CARLA to apply reinforcement learning algorithms such as Deep Q-Networks (DQN) and Proximal Policy Optimization (PPO) to teach agents how to drive safely, avoid obstacles, and interact with traffic. Another relevant area of work comes from DeepMind, which has used reinforcement learning to teach robots how to perform complex tasks by interacting with simulated environments. While their research focuses more on robotics, the techniques and successes they’ve had are very applicable to autonomous driving. Our project builds on those existing efforts by experimenting with RL algorithms in simulation, aiming to better understand how well these methods can handle real-world driving challenges like unpredictable traffic or sudden obstacles.\n",
    "\n",
    "\n",
    "*** NEED OUTSIDE SOURCES/REFERENCES\n",
    "\n",
    "Souces:\n",
    "* Mnih et al. (2015) – Deep Q-Networks (DQN)\n",
    "* Schulman et al. (2017) – Proximal Policy Optimization (PPO)\n",
    "* Dosovitskiy et al. (2017) – CARLA simulator\n",
    "* Sutton & Barto – RL textbook (for foundational definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refined the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There has been extensive work applying reinforcement learning (RL) to autonomous vehicles. Industry leaders such as Tesla and Waymo have pioneered systems capable of making real-time driving decisions using advanced machine learning. Tesla’s Autopilot leverages deep neural networks to detect lane lines, classify objects, and perform lane-change maneuvers in real time (Tesla AI Team, 2021). Waymo has taken a modular approach, combining deep learning and imitation learning to build fully autonomous vehicles that navigate complex urban environments (Waymo Team, 2020). Both companies rely on deep learning models for core control functions like steering, braking, and obstacle avoidance.\n",
    "\n",
    "In academic research, one of the most widely used platforms is CARLA (Car Learning to Act), an open-source simulator designed for training and testing autonomous driving agents (Dosovitskiy et al., 2017). CARLA provides a realistic environment for applying RL algorithms such as Deep Q-Networks (DQN) (Mnih et al., 2015) and Proximal Policy Optimization (PPO) (Schulman et al., 2017). These methods have been used to train agents to follow routes, avoid obstacles, and safely interact with traffic in various weather and traffic conditions.\n",
    "\n",
    "Additional inspiration comes from DeepMind’s work, which explores how reinforcement learning can be used to train robots through simulated environments (Silver et al., 2016). Although their work focuses more broadly on robotics and game-playing agents, the underlying techniques—especially regarding generalization and environment interaction—are highly relevant to the autonomous driving domain.\n",
    "\n",
    "Our project builds on these efforts by using RL algorithms within a simulation environment to investigate how well different techniques handle real-world driving challenges such as unpredictable traffic, pedestrians, and sudden obstacles. The goal is to understand which algorithms generalize best, and what limitations may arise when transferring policies from simulation to reality.\n",
    "\n",
    "**References**\n",
    "\n",
    "- Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. *Conference on Robot Learning (CoRL)*.\n",
    "- Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. *Nature*.\n",
    "- Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. *arXiv preprint arXiv:1707.06347*.\n",
    "- Silver, D., Huang, A., Maddison, C. J., et al. (2016). Mastering the game of Go with deep neural networks and tree search. *Nature*.\n",
    "- Tesla AI Team. (2021). *Tesla AI Day 2021*. [https://www.tesla.com/AI](https://www.tesla.com/AI)\n",
    "- Waymo Team. (2020). *On the Road to Fully Autonomous Driving*. [https://waymo.com/research](https://waymo.com/research)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.4 Main Research Questions**\n",
    "One of the main questions we’re exploring is: **How can reinforcement learning actually improve the decision-making process in autonomous vehicles, especially in unpredictable or complex situations?** We want to see whether RL can help AVs make smarter, safer real-time choices when things like traffic jams, unexpected pedestrians, or road closures come up.\n",
    "\n",
    "We’re also asking: **Which RL algorithms work best for teaching self-driving cars how to safely move through traffic and avoid obstacles?** To figure this out, we’ll be testing a few popular algorithms—like Q-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO)—to see how well they handle tasks like staying on course, avoiding crashes, and reacting to changes in the environment.\n",
    "\n",
    "Another important question is: **How can we evaluate whether these RL systems are ready for real-world use?** It’s one thing for a model to perform well in a simulator, but we need to think about how those skills transfer to actual roads. This includes looking at how safe and efficient they are, and how they handle tricky situations that involve ethical decisions—like what to do in an unavoidable accident scenario.\n",
    "\n",
    "Finally, we want to look at **what specific challenges come up when using RL in real-world autonomous driving, and how we might deal with them.** Since RL depends a lot on simulation, there’s always a risk that the agent won’t behave the same way outside of that controlled setting. Part of our work will focus on finding ways to close that gap between learning in simulation and driving in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refined above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This research investigates how reinforcement learning (RL) can enhance autonomous vehicle (AV) decision-making in complex, real-world environments. The following questions guide our inquiry:\n",
    "\n",
    "1. **How can reinforcement learning improve real-time decision-making in autonomous vehicles, particularly in dynamic or unpredictable conditions?**  \n",
    "   Autonomous vehicles must continuously interpret and respond to a wide range of stimuli—traffic congestion, sudden pedestrian crossings, erratic driver behavior, or unexpected road closures. We aim to assess whether RL-based control systems can enable AVs to make safer, more context-aware decisions in such scenarios, compared to traditional rule-based or supervised approaches.\n",
    "\n",
    "2. **Which RL algorithms are most effective for training autonomous vehicles to navigate traffic, avoid collisions, and adapt to changing environments?**  \n",
    "   To address this question, we will compare several widely studied RL algorithms—Q-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO)—in a simulated driving environment. These algorithms differ in their learning dynamics, representational capacity, and suitability for discrete versus continuous action spaces. By evaluating their performance across various tasks (e.g., lane-following, obstacle avoidance, dynamic rerouting), we seek to identify which approach best balances learning efficiency, stability, and safety.\n",
    "\n",
    "3. **How transferable are reinforcement learning policies trained in simulation to real-world driving conditions?**  \n",
    "   High performance in simulation does not guarantee successful deployment in the physical world. This question examines the “reality gap”: the extent to which learned policies generalize from synthetic to real-world domains. We will analyze agent behavior under unseen conditions (e.g., weather shifts, occluded objects, new traffic rules) to evaluate generalization, robustness, and the potential need for domain adaptation techniques.\n",
    "\n",
    "4. **What are the key limitations of using reinforcement learning in autonomous driving, and how can they be mitigated?**  \n",
    "   Despite its potential, RL presents unique challenges in the context of AVs, including sample inefficiency, safety during exploration, reward design complexity, and ethical ambiguity in edge-case scenarios (e.g., unavoidable collisions). Our work aims to surface these limitations and propose possible strategies for addressing them, such as incorporating safety constraints, curriculum learning, or hybrid models that combine RL with rule-based oversight.\n",
    "\n",
    "Together, these questions structure our investigation into how reinforcement learning can serve as a foundation for more intelligent, adaptable, and trustworthy autonomous driving systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.5 Our Hypothesis**\n",
    "We **hypothesize that Reinforcement Learning (RL) algorithms can significantly enhance the real-time decision-making capabilities of autonomous vehicles by enabling them to learn from past interactions with the environment and improve over time**. Specifically:\n",
    "\n",
    "- Deep Q-Learning Networks (DQN) will prove effective in improving basic tasks such as navigation and obstacle avoidance due to its ability to estimate Q-values and maximize long-term rewards.\n",
    "\n",
    "- Proximal Policy Optimization (PPO) will provide an efficient solution for training agents in large, continuous action spaces (e.g., steering, accelerating, braking) while ensuring stability in learning.\n",
    "\n",
    "The RL-trained agents will be able to navigate complex driving scenarios with a higher level of safety and efficiency compared to traditional, rule-based systems. However, there may be challenges related to transferring the learned behavior to real-world environments, especially in highly dynamic traffic conditions.\n",
    "\n",
    "We expect that RL-based autonomous vehicles will show improved performance in real-time decision-making tasks, but also face challenges in the scalability and robustness of learned behaviors when transitioning from simulated to real-world environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that **reinforcement learning (RL) algorithms can significantly improve the real-time decision-making capabilities of autonomous vehicles by enabling agents to learn from environmental interactions and iteratively refine their behavior over time.** \n",
    "\n",
    "Specifically, we anticipate the following:\n",
    "\n",
    "- **Deep Q-Networks (DQN)** will be effective for discrete, task-specific behaviors such as lane following and obstacle avoidance. DQN’s ability to approximate Q-values in high-dimensional state spaces allows it to optimize for long-term cumulative reward in structured environments.\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)** will demonstrate superior performance in more complex, continuous control tasks such as steering, acceleration, and braking. PPO’s stability and sample efficiency make it well-suited for learning policies in high-dimensional action spaces without diverging or overfitting.\n",
    "\n",
    "We expect that RL-trained agents will outperform traditional rule-based systems in both safety and efficiency metrics when navigating complex driving scenarios, including unpredictable traffic patterns and dynamically changing road conditions. However, we also anticipate significant challenges in transferring these learned policies from simulation to real-world settings. These include reduced generalization to novel scenarios, sensitivity to domain shift, and difficulties in maintaining robust behavior under rare or adversarial conditions.\n",
    "\n",
    "Overall, we hypothesize that while reinforcement learning offers a powerful framework for autonomous driving, its practical deployment will depend on solving key challenges related to scalability, reliability, and transferability across domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Methodology & Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- You have a great deal of free rein to decide what analyses you should use and therefore you will be graded on the appropriateness of the methods chosen. \n",
    "- Argue for a few analyses that can help to answer your research question(s). \n",
    "- You should argue for more than one kind of analysis to help answer your research questions. \n",
    "- Try to make this the first draft of your methodology. \n",
    "- This will eventually turn into 1-2 slides on methodology in your final presentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology & Analysis\n",
    "To investigate how reinforcement learning (RL) can improve real-time decision-making in autonomous vehicles, we will implement and compare three RL algorithms—Q-Learning, Deep Q-Networks (DQN), and Proximal Policy Optimization (PPO)—in a controlled simulation environment. Our methodology consists of three components: simulation design, algorithm implementation, and multi-faceted evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Simulation Environment\n",
    "\n",
    "All experiments will be conducted using CARLA, an open-source, high-fidelity urban driving simulator designed for autonomous vehicle research. CARLA offers configurable weather, lighting, traffic density, pedestrian movement, and road layouts, enabling us to evaluate learning agents under diverse and unpredictable driving conditions. Scenarios will include both structured tasks (e.g., lane following, navigation to a target location) and unstructured ones (e.g., obstacle avoidance in dynamic traffic).\n",
    "\n",
    "By using simulation, we ensure a reproducible, scalable, and risk-free setting in which to train agents over many episodes, including scenarios that would be unsafe or infeasible to test in real vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Reinforcement Learning Algorithms\n",
    "\n",
    "We will implement the following RL algorithms for comparative analysis:\n",
    "\n",
    "- **Q-Learning**: A tabular method suitable for discrete state and action spaces. While limited in scalability, it serves as a useful baseline to establish the difficulty of tasks and explore the constraints of simple value-based learning.\n",
    "  \n",
    "- **Deep Q-Networks (DQN)**: An extension of Q-learning that utilizes deep neural networks to approximate Q-values in high-dimensional state spaces. DQN is widely used in control problems and has shown strong performance in tasks involving perception and sequential decision-making.\n",
    "  \n",
    "- **Proximal Policy Optimization (PPO)**: A state-of-the-art policy gradient method known for its sample efficiency and training stability, particularly in continuous action environments such as vehicle control. PPO is expected to perform well in tasks requiring fine-grained control over steering, acceleration, and braking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Experimental Tasks\n",
    "\n",
    "To assess the generalizability and robustness of each algorithm, agents will be evaluated on a progression of tasks:\n",
    "\n",
    "1. **Basic Lane-Following**: Tests stability and path adherence under ideal conditions.\n",
    "2. **Dynamic Obstacle Avoidance**: Introduces moving vehicles and pedestrians.\n",
    "3. **Urban Navigation with Traffic Rules**: Requires the agent to follow stop signs, traffic lights, and make turn decisions.\n",
    "4. **Environmental Perturbation**: Evaluates policy robustness under rain, fog, and nighttime conditions.\n",
    "\n",
    "Each agent will be trained on a subset of tasks and tested on both seen and unseen conditions to measure generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluation Metrics\n",
    "\n",
    "Our analysis will include both **quantitative** and **qualitative** metrics:\n",
    "\n",
    "- **Safety**: Number of collisions, near-misses, or traffic violations per episode.\n",
    "- **Efficiency**: Time-to-completion and distance traveled.\n",
    "- **Reward Accumulation**: Cumulative reward across training and evaluation episodes.\n",
    "- **Policy Robustness**: Drop in performance when transferred to new or perturbed environments.\n",
    "- **Convergence Behavior**: Training stability and learning curves across episodes.\n",
    "\n",
    "These metrics will allow us to compare not only raw performance but also the reliability and adaptability of each RL approach under varying conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Analysis Strategy\n",
    "\n",
    "We will conduct both **intra-algorithm analysis** (comparing performance of each algorithm across tasks and conditions) and **inter-algorithm comparison** (benchmarking algorithms against one another). Additionally, visualizations such as reward curves, policy heatmaps, and behavioral traces will be used to illustrate differences in decision-making behavior.\n",
    "\n",
    "Qualitative insights (e.g., agent hesitation at intersections or failure under occlusion) will also be recorded through visual inspection and frame-by-frame analysis of edge cases. This will help identify specific limitations in the learned policies and suggest directions for future improvement (e.g., curriculum learning, hybrid models, or safety constraints).\n",
    "\n",
    "Together, these methods offer a comprehensive approach to evaluating the practical potential of reinforcement learning for autonomous vehicle decision-making in complex and variable environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Visualizations & Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5 Points] \n",
    "- Perform one part of the analysis to help answer one (or more) research question(s). \n",
    "- Create visualizations that will help to provide evidence. \n",
    "- Discuss the results and how they provide evidence for answering the research questions. \n",
    "- Try to make this a first draft of one part of the results for the project.\n",
    "- Try to have at least one visualization that you plan to use as a figure in the final presentation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
